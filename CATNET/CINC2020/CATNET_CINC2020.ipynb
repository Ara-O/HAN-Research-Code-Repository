{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "girUNxm63R2y",
        "outputId": "25b11448-406e-4339-b1e8-01967f55816b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m144.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q wfdb pywavelets torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZghIDfpg3OBK"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import wfdb\n",
        "from wfdb import get_record_list, dl_database\n",
        "import pywt\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import fbeta_score, roc_auc_score, roc_curve, roc_curve, auc\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLsLde-13DGY",
        "outputId": "67eed6a7-dbba-44ac-8651-6cf68d0f3834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkYkq1-s3GlP",
        "outputId": "8570a5b9-2dd1-4854-d0bf-1a3839405a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted /content/drive/MyDrive/CINC2020.zip to /content/CINC\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "# Define the path to the zip file in Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/CINC2020.zip'\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = '/content/CINC'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Check if the zip file exists\n",
        "if os.path.exists(zip_file_path):\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Successfully extracted {zip_file_path} to {extract_path}\")\n",
        "else:\n",
        "    print(f\"Error: {zip_file_path} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvc2Ww5J5Vxo"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "\n",
        "def denoise_ecg(signal, wavelet='db5', level=9):\n",
        "    \"\"\"\n",
        "    Denoise ECG signal (multi-lead) using wavelet transform.\n",
        "    signal: shape (n_leads, n_samples)\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(signal, wavelet, level=level, axis=1)\n",
        "    # Universal threshold\n",
        "    threshold = (np.median(np.abs(coeffs[-1])) / 0.6745) * (np.sqrt(2 * np.log(signal.shape[1])))\n",
        "\n",
        "    coeffs[-1].fill(0)\n",
        "    coeffs[-2].fill(0)\n",
        "\n",
        "    for i in range(1, len(coeffs) - 2):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
        "\n",
        "    rdata = pywt.waverec(coeffs, wavelet)\n",
        "\n",
        "    return np.nan_to_num(rdata, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "def pad_or_truncate(arr, target_len):\n",
        "    \"\"\"\n",
        "    Pad or truncate signal to fixed length.\n",
        "    arr: shape (n_leads, n_samples)\n",
        "    \"\"\"\n",
        "    if arr.shape[1] < target_len:\n",
        "        pad_width = target_len - arr.shape[1]\n",
        "        return np.pad(arr, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    return arr[:, :target_len]\n",
        "\n",
        "\n",
        "def process_record(file_name, folder, available_labels):\n",
        "    final_data_path = f\"CINC/classification-of-12-lead-ecgs-the-physionetcomputing-in-cardiology-challenge-2020-1.0.2/{folder}/{file_name}\"\n",
        "    hdr = wfdb.rdheader(final_data_path)\n",
        "\n",
        "    for comment in hdr.comments:\n",
        "        if comment.startswith('Dx:') and \",\" not in comment:\n",
        "            label = comment.replace('Dx: ', '')\n",
        "            if label not in available_labels:\n",
        "                return None\n",
        "\n",
        "            record = wfdb.rdrecord(final_data_path)\n",
        "            signal = np.nan_to_num(record.p_signal, nan=0.0)\n",
        "\n",
        "            signal = signal.T  # shape (n_leads, n_samples)\n",
        "\n",
        "            denoised = denoise_ecg(signal)  # vectorized\n",
        "            return denoised, label\n",
        "    return None\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    print(\"Loading and preprocessing ECG data...\")\n",
        "\n",
        "    RECORDS = \"CINC/classification-of-12-lead-ecgs-the-physionetcomputing-in-cardiology-challenge-2020-1.0.2/RECORDS\"\n",
        "    TARGET_LENGTH = 5000\n",
        "    available_labels = [\n",
        "        '10370003', '164889003', '164909002', '164934002',\n",
        "        '270492004', '284470004', '426177001', '426783006',\n",
        "        '427084000', '427393009', '59118001'\n",
        "    ]\n",
        "\n",
        "    X, Y = [], []\n",
        "\n",
        "    with open(RECORDS) as f:\n",
        "        folders_list = f.read()\n",
        "        FOLDERS = folders_list.strip().splitlines()\n",
        "\n",
        "    tasks = []\n",
        "    with ProcessPoolExecutor() as ex:\n",
        "        for folder in FOLDERS:\n",
        "            if folder in [\"training/ptb/g1/\", \"training/st_petersburg_incart/g1/\"]:\n",
        "                continue\n",
        "\n",
        "            with open(f\"CINC/classification-of-12-lead-ecgs-the-physionetcomputing-in-cardiology-challenge-2020-1.0.2/{folder}/RECORDS\") as r:\n",
        "                files = r.read().strip().splitlines()[:-1]\n",
        "\n",
        "            for file_name in files:\n",
        "                tasks.append(ex.submit(process_record, file_name, folder, available_labels))\n",
        "\n",
        "        results = [t.result() for t in tasks if t.result() is not None]\n",
        "\n",
        "    for signal, label in results:\n",
        "        truncated = pad_or_truncate(signal, TARGET_LENGTH)\n",
        "        X.append(truncated)\n",
        "        Y.append(label)\n",
        "\n",
        "    print(\"Data loading finished\")\n",
        "\n",
        "    # Convert to arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    label2idx = {label: i for i, label in enumerate(available_labels)}\n",
        "    Y_idx = np.array([label2idx[y] for y in Y])\n",
        "    Y_one_hot = np.eye(len(available_labels))[Y_idx]\n",
        "\n",
        "    return X, Y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epBPst7s5aOy",
        "outputId": "612e4a27-8e2d-4a3a-dc83-2b2c69fdb1d2"
      },
      "outputs": [],
      "source": [
        "standardized_arrays, Y_one_hot_final = load_and_preprocess_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yW9NnEZ9ygb"
      },
      "outputs": [],
      "source": [
        "standardized_arrays = standardized_arrays.transpose(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ewp5Tbj6m4b",
        "outputId": "cfe7a89c-ce6b-4042-cdd0-3861c2b728bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14530, 5000, 12), (14530, 11))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "standardized_arrays.shape, Y_one_hot_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvpgp8nH6-Pu"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(standardized_arrays, dtype=torch.float32)   # (14530, 5000, 1)\n",
        "y = torch.tensor(np.array(Y_one_hot_final).argmax(axis=1), dtype=torch.long)  # (14530,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxePcG-btGdN",
        "outputId": "d74eafb0-a823-47a8-de0c-9600d4199ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output shape: torch.Size([2, 11])\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Channel Attention (CBAM)\n",
        "# -------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, ratio=8):\n",
        "        super().__init__()\n",
        "        hidden = max(1, channels // ratio)\n",
        "        # Shared MLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(channels, hidden, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, channels, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        b, c, l = x.shape\n",
        "\n",
        "        # Global average pooling and max pooling across temporal dimension -> (B, C)\n",
        "        avg_pool = F.adaptive_avg_pool1d(x, 1).view(b, c)   # (B, C)\n",
        "        max_pool = F.adaptive_max_pool1d(x, 1).view(b, c)   # (B, C)\n",
        "\n",
        "        # Shared MLP applied to both\n",
        "        avg_out = self.mlp(avg_pool)  # (B, C)\n",
        "        max_out = self.mlp(max_pool)  # (B, C)\n",
        "\n",
        "        # Combine and sigmoid\n",
        "        out = torch.sigmoid(avg_out + max_out).unsqueeze(-1)  # (B, C, 1)\n",
        "\n",
        "        # Scale input\n",
        "        return x * out  # broadcasting over length\n",
        "\n",
        "# -------------------------\n",
        "# Spatial Attention (CBAM)\n",
        "# -------------------------\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv1d(in_channels=2, out_channels=1,\n",
        "                              kernel_size=kernel_size, padding=padding, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        # channel-wise avg and max -> (B, 1, L)\n",
        "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        cat = torch.cat([avg_pool, max_pool], dim=1)  # (B, 2, L)\n",
        "        attn = torch.sigmoid(self.conv(cat))  # (B, 1, L)\n",
        "        return x * attn  # broadcast over channels\n",
        "\n",
        "# -------------------------\n",
        "# Transformer Encoder Block\n",
        "# -------------------------\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # Use batch_first=True so inputs are (B, L, D)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads,\n",
        "                                         dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, x, src_mask=None, src_key_padding_mask=None):\n",
        "        # x: (B, L, D)\n",
        "        attn_out, _ = self.mha(x, x, x,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)\n",
        "        attn_out = self.dropout1(attn_out)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        ffn_out = self.dropout2(ffn_out)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x  # (B, L, D)\n",
        "\n",
        "# -------------------------\n",
        "# Positional Encoding (sinusoidal)\n",
        "# -------------------------\n",
        "def sinusoidal_positional_encoding(seq_len, d_model, device=None, dtype=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cpu')\n",
        "    pe = torch.zeros(seq_len, d_model, device=device, dtype=dtype)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device, dtype=torch.float)\n",
        "                         * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "\n",
        "# -------------------------\n",
        "# Full Model\n",
        "# -------------------------\n",
        "class ECGModel(nn.Module):\n",
        "    def __init__(self, sequence_length=5000, num_channels=12,\n",
        "                 d_model=128, num_heads=4, dff=128, dropout_rate=0.2,\n",
        "                 num_classes=11, apply_softmax=False):\n",
        "        super().__init__()\n",
        "        self.apply_softmax = apply_softmax\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=32, kernel_size=21, padding=(21-1)//2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.ca1 = ChannelAttention(32, ratio=8)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1) \n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 32, kernel_size=23, padding=(23-1)//2)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.ca2 = ChannelAttention(32, ratio=8)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(32, 64, kernel_size=25, padding=(25-1)//2)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.ca3 = ChannelAttention(64, ratio=8)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv4 = nn.Conv1d(64, 128, kernel_size=27, padding=(27-1)//2)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.ca4 = ChannelAttention(128, ratio=8)\n",
        "\n",
        "        if 128 != d_model:\n",
        "            self.project_to_d_model = nn.Conv1d(128, d_model, kernel_size=1)\n",
        "        else:\n",
        "            self.project_to_d_model = None\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.transformer_block = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads,\n",
        "                                                         dff=dff, dropout_rate=dropout_rate)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = None  \n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc_out = nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "        self._configured = False\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _init_fc_head(self, sample_batch, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            x = sample_batch.to(device)\n",
        "            features = self._forward_features(x) \n",
        "            flat = features.reshape(features.size(0), -1)\n",
        "            in_features = flat.size(1)\n",
        "            self.fc1 = nn.Linear(in_features, 128).to(device)\n",
        "            nn.init.xavier_uniform_(self.fc1.weight)\n",
        "            nn.init.zeros_(self.fc1.bias)\n",
        "            self._configured = True\n",
        "\n",
        "\n",
        "    def _forward_features(self, x):\n",
        "        x = x.permute(0, 2, 1)  # -> (B, C, L)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca4(x)\n",
        "        # x: (B, channels, L_final)\n",
        "\n",
        "        if self.project_to_d_model is not None:\n",
        "            x = self.project_to_d_model(x)  # (B, d_model, L_final)\n",
        "\n",
        "        # Transpose for transformer: (B, L, D)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        seq_len = x.size(1)\n",
        "        pe = sinusoidal_positional_encoding(seq_len, self.d_model, device=x.device, dtype=x.dtype)\n",
        "        x = x + pe  # (B, L, D)\n",
        "\n",
        "        # Transformer encoder block\n",
        "        x = self.transformer_block(x)  # (B, L, D)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "\n",
        "        if not self._configured:\n",
        "            self._init_fc_head(x, device)\n",
        "\n",
        "        features = self._forward_features(x)  # (B, L, D)\n",
        "        flat = features.reshape(features.size(0), -1)  # flatten\n",
        "        x = F.relu(self.fc1(flat))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc_out(x)\n",
        "        if self.apply_softmax:\n",
        "            return F.softmax(logits, dim=-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "sequence_length = 5000\n",
        "num_channels = 12\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGModel(sequence_length=sequence_length, num_channels=num_channels,\n",
        "                  d_model=128, num_heads=4, dff=128, dropout_rate=0.2,\n",
        "                  num_classes=11, apply_softmax=False).to(device)\n",
        "\n",
        "# dummy batch with shape (batch, seq_len, channels)\n",
        "dummy = torch.randn(2, sequence_length, num_channels, device=device)\n",
        "out = model(dummy)  # out shape: (2, 11) logits\n",
        "print(\"output shape:\", out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe7U5xaVAmLJ",
        "outputId": "baf9d01c-0357-4feb-8e3b-a1c1f369618f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([14530, 5000, 12])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ECGModel                                 [1, 11]                   --\n",
              "├─Conv1d: 1-1                            [1, 32, 5000]             8,096\n",
              "├─BatchNorm1d: 1-2                       [1, 32, 5000]             64\n",
              "├─ChannelAttention: 1-3                  [1, 32, 5000]             --\n",
              "│    └─Sequential: 2-1                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-1                  [1, 4]                    132\n",
              "│    │    └─ReLU: 3-2                    [1, 4]                    --\n",
              "│    │    └─Linear: 3-3                  [1, 32]                   160\n",
              "│    └─Sequential: 2-2                   [1, 32]                   (recursive)\n",
              "│    │    └─Linear: 3-4                  [1, 4]                    (recursive)\n",
              "│    │    └─ReLU: 3-5                    [1, 4]                    --\n",
              "│    │    └─Linear: 3-6                  [1, 32]                   (recursive)\n",
              "├─MaxPool1d: 1-4                         [1, 32, 2500]             --\n",
              "├─Conv1d: 1-5                            [1, 32, 2500]             23,584\n",
              "├─BatchNorm1d: 1-6                       [1, 32, 2500]             64\n",
              "├─ChannelAttention: 1-7                  [1, 32, 2500]             --\n",
              "│    └─Sequential: 2-3                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-7                  [1, 4]                    132\n",
              "│    │    └─ReLU: 3-8                    [1, 4]                    --\n",
              "│    │    └─Linear: 3-9                  [1, 32]                   160\n",
              "│    └─Sequential: 2-4                   [1, 32]                   (recursive)\n",
              "│    │    └─Linear: 3-10                 [1, 4]                    (recursive)\n",
              "│    │    └─ReLU: 3-11                   [1, 4]                    --\n",
              "│    │    └─Linear: 3-12                 [1, 32]                   (recursive)\n",
              "├─MaxPool1d: 1-8                         [1, 32, 1250]             --\n",
              "├─Conv1d: 1-9                            [1, 64, 1250]             51,264\n",
              "├─BatchNorm1d: 1-10                      [1, 64, 1250]             128\n",
              "├─ChannelAttention: 1-11                 [1, 64, 1250]             --\n",
              "│    └─Sequential: 2-5                   [1, 64]                   --\n",
              "│    │    └─Linear: 3-13                 [1, 8]                    520\n",
              "│    │    └─ReLU: 3-14                   [1, 8]                    --\n",
              "│    │    └─Linear: 3-15                 [1, 64]                   576\n",
              "│    └─Sequential: 2-6                   [1, 64]                   (recursive)\n",
              "│    │    └─Linear: 3-16                 [1, 8]                    (recursive)\n",
              "│    │    └─ReLU: 3-17                   [1, 8]                    --\n",
              "│    │    └─Linear: 3-18                 [1, 64]                   (recursive)\n",
              "├─MaxPool1d: 1-12                        [1, 64, 625]              --\n",
              "├─Conv1d: 1-13                           [1, 128, 625]             221,312\n",
              "├─BatchNorm1d: 1-14                      [1, 128, 625]             256\n",
              "├─ChannelAttention: 1-15                 [1, 128, 625]             --\n",
              "│    └─Sequential: 2-7                   [1, 128]                  --\n",
              "│    │    └─Linear: 3-19                 [1, 16]                   2,064\n",
              "│    │    └─ReLU: 3-20                   [1, 16]                   --\n",
              "│    │    └─Linear: 3-21                 [1, 128]                  2,176\n",
              "│    └─Sequential: 2-8                   [1, 128]                  (recursive)\n",
              "│    │    └─Linear: 3-22                 [1, 16]                   (recursive)\n",
              "│    │    └─ReLU: 3-23                   [1, 16]                   --\n",
              "│    │    └─Linear: 3-24                 [1, 128]                  (recursive)\n",
              "├─TransformerEncoderBlock: 1-16          [1, 625, 128]             --\n",
              "│    └─MultiheadAttention: 2-9           [1, 625, 128]             66,048\n",
              "│    └─Dropout: 2-10                     [1, 625, 128]             --\n",
              "│    └─LayerNorm: 2-11                   [1, 625, 128]             256\n",
              "│    └─Sequential: 2-12                  [1, 625, 128]             --\n",
              "│    │    └─Linear: 3-25                 [1, 625, 128]             16,512\n",
              "│    │    └─ReLU: 3-26                   [1, 625, 128]             --\n",
              "│    │    └─Linear: 3-27                 [1, 625, 128]             16,512\n",
              "│    └─Dropout: 2-13                     [1, 625, 128]             --\n",
              "│    └─LayerNorm: 2-14                   [1, 625, 128]             256\n",
              "├─Linear: 1-17                           [1, 128]                  10,240,128\n",
              "├─Dropout: 1-18                          [1, 128]                  --\n",
              "├─Linear: 1-19                           [1, 11]                   1,419\n",
              "==========================================================================================\n",
              "Total params: 10,651,819\n",
              "Trainable params: 10,651,819\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 312.13\n",
              "==========================================================================================\n",
              "Input size (MB): 0.24\n",
              "Forward/backward pass size (MB): 8.97\n",
              "Params size (MB): 42.34\n",
              "Estimated Total Size (MB): 51.55\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "dummy_input = torch.randn(1, X.shape[1], X.shape[2]).to(device)\n",
        "\n",
        "summary(model, input_data=dummy_input, device=str(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu3MNCGAK32m"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
        "import random\n",
        "\n",
        "def evaluate_on_loader(model, loader, criterion, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels, all_preds, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            logits = model(xb)  # (B, C)\n",
        "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "               print(f\"Found NaN/Inf in logits at batch {i}\")\n",
        "               any_nan = True\n",
        "            loss = criterion(logits, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            labels = yb.cpu().numpy()\n",
        "\n",
        "            all_labels.extend(labels)\n",
        "            all_preds.extend(preds)\n",
        "            all_probs.extend(probs)\n",
        "\n",
        "    avg_loss = running_loss / len(all_labels)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "\n",
        "    try:\n",
        "        y_true = np.eye(num_classes)[all_labels]\n",
        "        y_score = np.array(all_probs)\n",
        "\n",
        "        auc_list = []\n",
        "        for i in range(num_classes):\n",
        "            if np.any(y_true[:, i]):  # class i exists\n",
        "                auc_list.append(roc_auc_score(y_true[:, i], y_score[:, i]))\n",
        "        if auc_list:\n",
        "            auc = np.mean(auc_list)\n",
        "        else:\n",
        "            auc = float(\"nan\")\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return avg_loss, acc, precision, recall, auc, f1\n",
        "\n",
        "\n",
        "def confusion_matrix(preds, targets, num_classes):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for p, t in zip(preds, targets):\n",
        "        cm[int(t), int(p)] += 1\n",
        "    return cm\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KycUMdh4uaWb",
        "outputId": "fe1fa315-23d3-45e6-df4a-0b1d1026b12d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== SEED 1: 0 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7327, Val Loss: 0.8637, Val Acc: 0.7699, Val F1: 0.7072\n",
            "Epoch 2/20 - Train Acc: 0.7858, Val Loss: 0.6245, Val Acc: 0.8234, Val F1: 0.7927\n",
            "Epoch 3/20 - Train Acc: 0.8349, Val Loss: 0.5018, Val Acc: 0.8486, Val F1: 0.8320\n",
            "Epoch 4/20 - Train Acc: 0.8543, Val Loss: 0.4735, Val Acc: 0.8624, Val F1: 0.8475\n",
            "Epoch 5/20 - Train Acc: 0.8665, Val Loss: 0.4823, Val Acc: 0.8563, Val F1: 0.8366\n",
            "Epoch 6/20 - Train Acc: 0.8755, Val Loss: 0.5249, Val Acc: 0.8341, Val F1: 0.8365\n",
            "Epoch 7/20 - Train Acc: 0.8892, Val Loss: 0.4489, Val Acc: 0.8685, Val F1: 0.8610\n",
            "Epoch 8/20 - Train Acc: 0.8921, Val Loss: 0.4293, Val Acc: 0.8777, Val F1: 0.8696\n",
            "Epoch 9/20 - Train Acc: 0.8959, Val Loss: 0.4138, Val Acc: 0.8838, Val F1: 0.8801\n",
            "Epoch 10/20 - Train Acc: 0.9017, Val Loss: 0.4430, Val Acc: 0.8899, Val F1: 0.8792\n",
            "Epoch 11/20 - Train Acc: 0.9094, Val Loss: 0.4570, Val Acc: 0.8746, Val F1: 0.8669\n",
            "Epoch 12/20 - Train Acc: 0.9127, Val Loss: 0.4757, Val Acc: 0.8784, Val F1: 0.8710\n",
            "Epoch 13/20 - Train Acc: 0.9283, Val Loss: 0.4141, Val Acc: 0.8876, Val F1: 0.8847\n",
            "Epoch 14/20 - Train Acc: 0.9390, Val Loss: 0.4003, Val Acc: 0.8998, Val F1: 0.8955\n",
            "Epoch 15/20 - Train Acc: 0.9406, Val Loss: 0.4331, Val Acc: 0.8945, Val F1: 0.8864\n",
            "Epoch 16/20 - Train Acc: 0.9423, Val Loss: 0.4623, Val Acc: 0.8983, Val F1: 0.8929\n",
            "Epoch 17/20 - Train Acc: 0.9493, Val Loss: 0.4628, Val Acc: 0.8968, Val F1: 0.8926\n",
            "Epoch 18/20 - Train Acc: 0.9619, Val Loss: 0.4446, Val Acc: 0.8968, Val F1: 0.8920\n",
            "Epoch 19/20 - Train Acc: 0.9651, Val Loss: 0.4700, Val Acc: 0.8983, Val F1: 0.8895\n",
            "Epoch 20/20 - Train Acc: 0.9679, Val Loss: 0.6276, Val Acc: 0.8517, Val F1: 0.8532\n",
            "Seed 0 | test_loss: 0.6032 | test_acc: 0.8445 | AUC: 0.9509 | Precision: 0.8617 | Recall: 0.8445 | F1: 0.8479\n",
            "\n",
            "===== SEED 2: 1 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7334, Val Loss: 0.8148, Val Acc: 0.7783, Val F1: 0.7197\n",
            "Epoch 2/20 - Train Acc: 0.7818, Val Loss: 0.5882, Val Acc: 0.8203, Val F1: 0.7919\n",
            "Epoch 3/20 - Train Acc: 0.8304, Val Loss: 0.4733, Val Acc: 0.8647, Val F1: 0.8398\n",
            "Epoch 4/20 - Train Acc: 0.8588, Val Loss: 0.6155, Val Acc: 0.7729, Val F1: 0.7907\n",
            "Epoch 5/20 - Train Acc: 0.8709, Val Loss: 0.4655, Val Acc: 0.8716, Val F1: 0.8549\n",
            "Epoch 6/20 - Train Acc: 0.8789, Val Loss: 0.4144, Val Acc: 0.8716, Val F1: 0.8666\n",
            "Epoch 7/20 - Train Acc: 0.8918, Val Loss: 0.3879, Val Acc: 0.8838, Val F1: 0.8774\n",
            "Epoch 8/20 - Train Acc: 0.8991, Val Loss: 0.4329, Val Acc: 0.8716, Val F1: 0.8671\n",
            "Epoch 9/20 - Train Acc: 0.9068, Val Loss: 0.4041, Val Acc: 0.8968, Val F1: 0.8844\n",
            "Epoch 10/20 - Train Acc: 0.9081, Val Loss: 0.3812, Val Acc: 0.8922, Val F1: 0.8876\n",
            "Epoch 11/20 - Train Acc: 0.9170, Val Loss: 0.3667, Val Acc: 0.8960, Val F1: 0.8864\n",
            "Epoch 12/20 - Train Acc: 0.9227, Val Loss: 0.3833, Val Acc: 0.8953, Val F1: 0.8862\n",
            "Epoch 13/20 - Train Acc: 0.9241, Val Loss: 0.3673, Val Acc: 0.8922, Val F1: 0.8845\n",
            "Epoch 14/20 - Train Acc: 0.9295, Val Loss: 0.4017, Val Acc: 0.8991, Val F1: 0.8913\n",
            "Epoch 15/20 - Train Acc: 0.9500, Val Loss: 0.4605, Val Acc: 0.8853, Val F1: 0.8825\n",
            "Epoch 16/20 - Train Acc: 0.9560, Val Loss: 0.4634, Val Acc: 0.8876, Val F1: 0.8865\n",
            "Epoch 17/20 - Train Acc: 0.9578, Val Loss: 0.4240, Val Acc: 0.8998, Val F1: 0.8927\n",
            "Epoch 18/20 - Train Acc: 0.9709, Val Loss: 0.4279, Val Acc: 0.8937, Val F1: 0.8906\n",
            "Epoch 19/20 - Train Acc: 0.9741, Val Loss: 0.4796, Val Acc: 0.8983, Val F1: 0.8933\n",
            "Epoch 20/20 - Train Acc: 0.9750, Val Loss: 0.5136, Val Acc: 0.8914, Val F1: 0.8852\n",
            "Seed 1 | test_loss: 0.5262 | test_acc: 0.8933 | AUC: 0.9536 | Precision: 0.8926 | Recall: 0.8933 | F1: 0.8909\n",
            "\n",
            "===== SEED 3: 2 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7284, Val Loss: 0.8169, Val Acc: 0.7783, Val F1: 0.7107\n",
            "Epoch 2/20 - Train Acc: 0.7791, Val Loss: 0.5717, Val Acc: 0.8356, Val F1: 0.7973\n",
            "Epoch 3/20 - Train Acc: 0.8372, Val Loss: 0.4818, Val Acc: 0.8677, Val F1: 0.8463\n",
            "Epoch 4/20 - Train Acc: 0.8633, Val Loss: 0.5697, Val Acc: 0.7997, Val F1: 0.8236\n",
            "Epoch 5/20 - Train Acc: 0.8752, Val Loss: 0.4044, Val Acc: 0.8769, Val F1: 0.8678\n",
            "Epoch 6/20 - Train Acc: 0.8841, Val Loss: 0.4567, Val Acc: 0.8677, Val F1: 0.8506\n",
            "Epoch 7/20 - Train Acc: 0.8891, Val Loss: 0.3905, Val Acc: 0.8815, Val F1: 0.8709\n",
            "Epoch 8/20 - Train Acc: 0.9002, Val Loss: 0.3740, Val Acc: 0.8930, Val F1: 0.8846\n",
            "Epoch 9/20 - Train Acc: 0.9086, Val Loss: 0.3993, Val Acc: 0.8891, Val F1: 0.8816\n",
            "Epoch 10/20 - Train Acc: 0.9125, Val Loss: 0.4292, Val Acc: 0.8838, Val F1: 0.8772\n",
            "Epoch 11/20 - Train Acc: 0.9142, Val Loss: 0.4007, Val Acc: 0.8922, Val F1: 0.8822\n",
            "Epoch 12/20 - Train Acc: 0.9386, Val Loss: 0.3844, Val Acc: 0.8991, Val F1: 0.8933\n",
            "Epoch 13/20 - Train Acc: 0.9398, Val Loss: 0.3793, Val Acc: 0.8983, Val F1: 0.8953\n",
            "Epoch 14/20 - Train Acc: 0.9457, Val Loss: 0.4987, Val Acc: 0.8754, Val F1: 0.8742\n",
            "Epoch 15/20 - Train Acc: 0.9544, Val Loss: 0.3895, Val Acc: 0.8922, Val F1: 0.8890\n",
            "Epoch 16/20 - Train Acc: 0.9624, Val Loss: 0.4172, Val Acc: 0.8968, Val F1: 0.8911\n",
            "Epoch 17/20 - Train Acc: 0.9661, Val Loss: 0.4588, Val Acc: 0.8953, Val F1: 0.8897\n",
            "Epoch 18/20 - Train Acc: 0.9705, Val Loss: 0.4549, Val Acc: 0.8968, Val F1: 0.8923\n",
            "Epoch 19/20 - Train Acc: 0.9720, Val Loss: 0.4538, Val Acc: 0.8998, Val F1: 0.8955\n",
            "Epoch 20/20 - Train Acc: 0.9739, Val Loss: 0.4628, Val Acc: 0.9006, Val F1: 0.8976\n",
            "Seed 2 | test_loss: 0.4624 | test_acc: 0.9126 | AUC: 0.9576 | Precision: 0.9008 | Recall: 0.9126 | F1: 0.9058\n",
            "\n",
            "===== SEED 4: 3 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7298, Val Loss: 0.8060, Val Acc: 0.7798, Val F1: 0.7141\n",
            "Epoch 2/20 - Train Acc: 0.7815, Val Loss: 0.6034, Val Acc: 0.8165, Val F1: 0.7658\n",
            "Epoch 3/20 - Train Acc: 0.8338, Val Loss: 0.5677, Val Acc: 0.8142, Val F1: 0.8074\n",
            "Epoch 4/20 - Train Acc: 0.8480, Val Loss: 0.4247, Val Acc: 0.8654, Val F1: 0.8469\n",
            "Epoch 5/20 - Train Acc: 0.8667, Val Loss: 0.5022, Val Acc: 0.8555, Val F1: 0.8375\n",
            "Epoch 6/20 - Train Acc: 0.8778, Val Loss: 0.4037, Val Acc: 0.8830, Val F1: 0.8757\n",
            "Epoch 7/20 - Train Acc: 0.8848, Val Loss: 0.4198, Val Acc: 0.8784, Val F1: 0.8657\n",
            "Epoch 8/20 - Train Acc: 0.8912, Val Loss: 0.4007, Val Acc: 0.8823, Val F1: 0.8678\n",
            "Epoch 9/20 - Train Acc: 0.9012, Val Loss: 0.3477, Val Acc: 0.8968, Val F1: 0.8873\n",
            "Epoch 10/20 - Train Acc: 0.8989, Val Loss: 0.3581, Val Acc: 0.8953, Val F1: 0.8892\n",
            "Epoch 11/20 - Train Acc: 0.9090, Val Loss: 0.3965, Val Acc: 0.8884, Val F1: 0.8751\n",
            "Epoch 12/20 - Train Acc: 0.9170, Val Loss: 0.3718, Val Acc: 0.8968, Val F1: 0.8916\n",
            "Epoch 13/20 - Train Acc: 0.9361, Val Loss: 0.3919, Val Acc: 0.8976, Val F1: 0.8927\n",
            "Epoch 14/20 - Train Acc: 0.9398, Val Loss: 0.3848, Val Acc: 0.8891, Val F1: 0.8876\n",
            "Epoch 15/20 - Train Acc: 0.9429, Val Loss: 0.3698, Val Acc: 0.8998, Val F1: 0.8972\n",
            "Epoch 16/20 - Train Acc: 0.9577, Val Loss: 0.3810, Val Acc: 0.9029, Val F1: 0.8977\n",
            "Epoch 17/20 - Train Acc: 0.9607, Val Loss: 0.3951, Val Acc: 0.9098, Val F1: 0.9044\n",
            "Epoch 18/20 - Train Acc: 0.9641, Val Loss: 0.3789, Val Acc: 0.9098, Val F1: 0.9050\n",
            "Epoch 19/20 - Train Acc: 0.9698, Val Loss: 0.4091, Val Acc: 0.9044, Val F1: 0.8997\n",
            "Epoch 20/20 - Train Acc: 0.9725, Val Loss: 0.4308, Val Acc: 0.9067, Val F1: 0.9027\n",
            "Seed 3 | test_loss: 0.4980 | test_acc: 0.8933 | AUC: 0.9648 | Precision: 0.8851 | Recall: 0.8933 | F1: 0.8883\n",
            "\n",
            "===== SEED 5: 4 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7246, Val Loss: 0.8148, Val Acc: 0.7653, Val F1: 0.6957\n",
            "Epoch 2/20 - Train Acc: 0.7702, Val Loss: 0.5742, Val Acc: 0.8310, Val F1: 0.7887\n",
            "Epoch 3/20 - Train Acc: 0.8273, Val Loss: 0.4643, Val Acc: 0.8677, Val F1: 0.8374\n",
            "Epoch 4/20 - Train Acc: 0.8545, Val Loss: 0.4079, Val Acc: 0.8838, Val F1: 0.8691\n",
            "Epoch 5/20 - Train Acc: 0.8708, Val Loss: 0.4361, Val Acc: 0.8823, Val F1: 0.8663\n",
            "Epoch 6/20 - Train Acc: 0.8821, Val Loss: 0.4150, Val Acc: 0.8869, Val F1: 0.8695\n",
            "Epoch 7/20 - Train Acc: 0.8899, Val Loss: 0.3957, Val Acc: 0.8777, Val F1: 0.8699\n",
            "Epoch 8/20 - Train Acc: 0.8982, Val Loss: 0.3560, Val Acc: 0.8983, Val F1: 0.8894\n",
            "Epoch 9/20 - Train Acc: 0.9012, Val Loss: 0.3999, Val Acc: 0.8899, Val F1: 0.8801\n",
            "Epoch 10/20 - Train Acc: 0.9075, Val Loss: 0.3876, Val Acc: 0.8953, Val F1: 0.8841\n",
            "Epoch 11/20 - Train Acc: 0.9137, Val Loss: 0.4456, Val Acc: 0.8761, Val F1: 0.8684\n",
            "Epoch 12/20 - Train Acc: 0.9327, Val Loss: 0.3852, Val Acc: 0.9014, Val F1: 0.8973\n",
            "Epoch 13/20 - Train Acc: 0.9380, Val Loss: 0.4013, Val Acc: 0.9044, Val F1: 0.8972\n",
            "Epoch 14/20 - Train Acc: 0.9436, Val Loss: 0.3917, Val Acc: 0.9044, Val F1: 0.8992\n",
            "Epoch 15/20 - Train Acc: 0.9582, Val Loss: 0.4319, Val Acc: 0.9006, Val F1: 0.8948\n",
            "Epoch 16/20 - Train Acc: 0.9628, Val Loss: 0.4457, Val Acc: 0.9021, Val F1: 0.8966\n",
            "Epoch 17/20 - Train Acc: 0.9641, Val Loss: 0.4617, Val Acc: 0.9006, Val F1: 0.8989\n",
            "Epoch 18/20 - Train Acc: 0.9723, Val Loss: 0.4505, Val Acc: 0.9067, Val F1: 0.9026\n",
            "Epoch 19/20 - Train Acc: 0.9764, Val Loss: 0.4716, Val Acc: 0.9067, Val F1: 0.9006\n",
            "Epoch 20/20 - Train Acc: 0.9767, Val Loss: 0.5087, Val Acc: 0.8998, Val F1: 0.8978\n",
            "Seed 4 | test_loss: 0.4854 | test_acc: 0.8995 | AUC: 0.9696 | Precision: 0.8970 | Recall: 0.8995 | F1: 0.8979\n",
            "\n",
            "===== SEED 6: 5 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7302, Val Loss: 0.9640, Val Acc: 0.7569, Val F1: 0.6795\n",
            "Epoch 2/20 - Train Acc: 0.7611, Val Loss: 0.7996, Val Acc: 0.8012, Val F1: 0.7387\n",
            "Epoch 3/20 - Train Acc: 0.8207, Val Loss: 0.5684, Val Acc: 0.8326, Val F1: 0.8047\n",
            "Epoch 4/20 - Train Acc: 0.8502, Val Loss: 0.4699, Val Acc: 0.8693, Val F1: 0.8604\n",
            "Epoch 5/20 - Train Acc: 0.8717, Val Loss: 0.5123, Val Acc: 0.8578, Val F1: 0.8520\n",
            "Epoch 6/20 - Train Acc: 0.8808, Val Loss: 0.4436, Val Acc: 0.8815, Val F1: 0.8719\n",
            "Epoch 7/20 - Train Acc: 0.8884, Val Loss: 0.4765, Val Acc: 0.8578, Val F1: 0.8472\n",
            "Epoch 8/20 - Train Acc: 0.8947, Val Loss: 0.4479, Val Acc: 0.8761, Val F1: 0.8687\n",
            "Epoch 9/20 - Train Acc: 0.8981, Val Loss: 0.4230, Val Acc: 0.8899, Val F1: 0.8813\n",
            "Epoch 10/20 - Train Acc: 0.9075, Val Loss: 0.4049, Val Acc: 0.8899, Val F1: 0.8837\n",
            "Epoch 11/20 - Train Acc: 0.9129, Val Loss: 0.3854, Val Acc: 0.8914, Val F1: 0.8877\n",
            "Epoch 12/20 - Train Acc: 0.9186, Val Loss: 0.4194, Val Acc: 0.8807, Val F1: 0.8738\n",
            "Epoch 13/20 - Train Acc: 0.9171, Val Loss: 0.4231, Val Acc: 0.8876, Val F1: 0.8829\n",
            "Epoch 14/20 - Train Acc: 0.9225, Val Loss: 0.3776, Val Acc: 0.8937, Val F1: 0.8904\n",
            "Epoch 15/20 - Train Acc: 0.9278, Val Loss: 0.3804, Val Acc: 0.8983, Val F1: 0.8919\n",
            "Epoch 16/20 - Train Acc: 0.9302, Val Loss: 0.5220, Val Acc: 0.8341, Val F1: 0.8473\n",
            "Epoch 17/20 - Train Acc: 0.9311, Val Loss: 0.4062, Val Acc: 0.8891, Val F1: 0.8870\n",
            "Epoch 18/20 - Train Acc: 0.9517, Val Loss: 0.3943, Val Acc: 0.8976, Val F1: 0.8938\n",
            "Epoch 19/20 - Train Acc: 0.9574, Val Loss: 0.4322, Val Acc: 0.8930, Val F1: 0.8917\n",
            "Epoch 20/20 - Train Acc: 0.9555, Val Loss: 0.4416, Val Acc: 0.8953, Val F1: 0.8907\n",
            "Seed 5 | test_loss: 0.4650 | test_acc: 0.8913 | AUC: 0.9711 | Precision: 0.8893 | Recall: 0.8913 | F1: 0.8889\n",
            "\n",
            "===== SEED 7: 6 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7214, Val Loss: 0.8041, Val Acc: 0.7920, Val F1: 0.7381\n",
            "Epoch 2/20 - Train Acc: 0.7841, Val Loss: 0.6656, Val Acc: 0.8188, Val F1: 0.7856\n",
            "Epoch 3/20 - Train Acc: 0.8252, Val Loss: 0.5234, Val Acc: 0.8547, Val F1: 0.8311\n",
            "Epoch 4/20 - Train Acc: 0.8547, Val Loss: 0.4552, Val Acc: 0.8631, Val F1: 0.8558\n",
            "Epoch 5/20 - Train Acc: 0.8749, Val Loss: 0.4849, Val Acc: 0.8677, Val F1: 0.8546\n",
            "Epoch 6/20 - Train Acc: 0.8809, Val Loss: 0.5736, Val Acc: 0.8104, Val F1: 0.8164\n",
            "Epoch 7/20 - Train Acc: 0.8868, Val Loss: 0.4419, Val Acc: 0.8716, Val F1: 0.8651\n",
            "Epoch 8/20 - Train Acc: 0.8946, Val Loss: 0.4646, Val Acc: 0.8746, Val F1: 0.8649\n",
            "Epoch 9/20 - Train Acc: 0.8986, Val Loss: 0.4250, Val Acc: 0.8922, Val F1: 0.8800\n",
            "Epoch 10/20 - Train Acc: 0.9129, Val Loss: 0.4719, Val Acc: 0.8815, Val F1: 0.8730\n",
            "Epoch 11/20 - Train Acc: 0.9122, Val Loss: 0.4328, Val Acc: 0.8807, Val F1: 0.8693\n",
            "Epoch 12/20 - Train Acc: 0.9162, Val Loss: 0.4495, Val Acc: 0.8754, Val F1: 0.8681\n",
            "Epoch 13/20 - Train Acc: 0.9338, Val Loss: 0.4366, Val Acc: 0.8853, Val F1: 0.8817\n",
            "Epoch 14/20 - Train Acc: 0.9411, Val Loss: 0.4272, Val Acc: 0.8876, Val F1: 0.8860\n",
            "Epoch 15/20 - Train Acc: 0.9464, Val Loss: 0.4702, Val Acc: 0.8853, Val F1: 0.8818\n",
            "Epoch 16/20 - Train Acc: 0.9573, Val Loss: 0.4565, Val Acc: 0.8960, Val F1: 0.8937\n",
            "Epoch 17/20 - Train Acc: 0.9645, Val Loss: 0.4656, Val Acc: 0.8991, Val F1: 0.8950\n",
            "Epoch 18/20 - Train Acc: 0.9673, Val Loss: 0.4736, Val Acc: 0.8960, Val F1: 0.8930\n",
            "Epoch 19/20 - Train Acc: 0.9745, Val Loss: 0.4990, Val Acc: 0.8960, Val F1: 0.8933\n",
            "Epoch 20/20 - Train Acc: 0.9767, Val Loss: 0.5066, Val Acc: 0.8976, Val F1: 0.8956\n",
            "Seed 6 | test_loss: 0.5243 | test_acc: 0.8995 | AUC: 0.9623 | Precision: 0.8980 | Recall: 0.8995 | F1: 0.8974\n",
            "\n",
            "===== SEED 8: 7 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7355, Val Loss: 0.8350, Val Acc: 0.7768, Val F1: 0.7107\n",
            "Epoch 2/20 - Train Acc: 0.7727, Val Loss: 0.6661, Val Acc: 0.8035, Val F1: 0.7504\n",
            "Epoch 3/20 - Train Acc: 0.8304, Val Loss: 0.5712, Val Acc: 0.8249, Val F1: 0.8094\n",
            "Epoch 4/20 - Train Acc: 0.8549, Val Loss: 0.5154, Val Acc: 0.8425, Val F1: 0.8405\n",
            "Epoch 5/20 - Train Acc: 0.8657, Val Loss: 0.4481, Val Acc: 0.8769, Val F1: 0.8688\n",
            "Epoch 6/20 - Train Acc: 0.8792, Val Loss: 0.4636, Val Acc: 0.8624, Val F1: 0.8573\n",
            "Epoch 7/20 - Train Acc: 0.8867, Val Loss: 0.5142, Val Acc: 0.8616, Val F1: 0.8420\n",
            "Epoch 8/20 - Train Acc: 0.8945, Val Loss: 0.4505, Val Acc: 0.8586, Val F1: 0.8543\n",
            "Epoch 9/20 - Train Acc: 0.9107, Val Loss: 0.4532, Val Acc: 0.8846, Val F1: 0.8791\n",
            "Epoch 10/20 - Train Acc: 0.9216, Val Loss: 0.4495, Val Acc: 0.8631, Val F1: 0.8612\n",
            "Epoch 11/20 - Train Acc: 0.9301, Val Loss: 0.4680, Val Acc: 0.8792, Val F1: 0.8753\n",
            "Epoch 12/20 - Train Acc: 0.9435, Val Loss: 0.4698, Val Acc: 0.8769, Val F1: 0.8769\n",
            "Epoch 13/20 - Train Acc: 0.9497, Val Loss: 0.5011, Val Acc: 0.8937, Val F1: 0.8857\n",
            "Epoch 14/20 - Train Acc: 0.9514, Val Loss: 0.5056, Val Acc: 0.8838, Val F1: 0.8774\n",
            "Epoch 15/20 - Train Acc: 0.9604, Val Loss: 0.5363, Val Acc: 0.8769, Val F1: 0.8744\n",
            "Epoch 16/20 - Train Acc: 0.9655, Val Loss: 0.5505, Val Acc: 0.8899, Val F1: 0.8817\n",
            "Epoch 17/20 - Train Acc: 0.9664, Val Loss: 0.5590, Val Acc: 0.8761, Val F1: 0.8738\n",
            "Epoch 18/20 - Train Acc: 0.9700, Val Loss: 0.5778, Val Acc: 0.8830, Val F1: 0.8773\n",
            "Epoch 19/20 - Train Acc: 0.9712, Val Loss: 0.5766, Val Acc: 0.8823, Val F1: 0.8779\n",
            "Epoch 20/20 - Train Acc: 0.9732, Val Loss: 0.6063, Val Acc: 0.8823, Val F1: 0.8782\n",
            "Seed 7 | test_loss: 0.5195 | test_acc: 0.8940 | AUC: 0.9576 | Precision: 0.8875 | Recall: 0.8940 | F1: 0.8895\n",
            "\n",
            "===== SEED 9: 8 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7325, Val Loss: 0.8292, Val Acc: 0.7859, Val F1: 0.7199\n",
            "Epoch 2/20 - Train Acc: 0.7721, Val Loss: 0.7122, Val Acc: 0.7951, Val F1: 0.7606\n",
            "Epoch 3/20 - Train Acc: 0.8236, Val Loss: 0.6197, Val Acc: 0.8249, Val F1: 0.7961\n",
            "Epoch 4/20 - Train Acc: 0.8530, Val Loss: 0.5672, Val Acc: 0.8096, Val F1: 0.8153\n",
            "Epoch 5/20 - Train Acc: 0.8671, Val Loss: 0.4945, Val Acc: 0.8631, Val F1: 0.8396\n",
            "Epoch 6/20 - Train Acc: 0.8756, Val Loss: 0.4644, Val Acc: 0.8654, Val F1: 0.8501\n",
            "Epoch 7/20 - Train Acc: 0.8871, Val Loss: 0.4466, Val Acc: 0.8639, Val F1: 0.8569\n",
            "Epoch 8/20 - Train Acc: 0.8914, Val Loss: 0.4246, Val Acc: 0.8716, Val F1: 0.8654\n",
            "Epoch 9/20 - Train Acc: 0.8975, Val Loss: 0.4537, Val Acc: 0.8731, Val F1: 0.8635\n",
            "Epoch 10/20 - Train Acc: 0.9065, Val Loss: 0.4970, Val Acc: 0.8754, Val F1: 0.8609\n",
            "Epoch 11/20 - Train Acc: 0.9065, Val Loss: 0.4510, Val Acc: 0.8807, Val F1: 0.8759\n",
            "Epoch 12/20 - Train Acc: 0.9287, Val Loss: 0.4389, Val Acc: 0.8784, Val F1: 0.8749\n",
            "Epoch 13/20 - Train Acc: 0.9361, Val Loss: 0.4532, Val Acc: 0.8846, Val F1: 0.8804\n",
            "Epoch 14/20 - Train Acc: 0.9397, Val Loss: 0.4220, Val Acc: 0.9014, Val F1: 0.8946\n",
            "Epoch 15/20 - Train Acc: 0.9428, Val Loss: 0.5178, Val Acc: 0.8708, Val F1: 0.8706\n",
            "Epoch 16/20 - Train Acc: 0.9466, Val Loss: 0.4729, Val Acc: 0.8846, Val F1: 0.8837\n",
            "Epoch 17/20 - Train Acc: 0.9463, Val Loss: 0.4406, Val Acc: 0.8899, Val F1: 0.8869\n",
            "Epoch 18/20 - Train Acc: 0.9628, Val Loss: 0.4558, Val Acc: 0.8930, Val F1: 0.8903\n",
            "Epoch 19/20 - Train Acc: 0.9668, Val Loss: 0.4774, Val Acc: 0.8884, Val F1: 0.8862\n",
            "Epoch 20/20 - Train Acc: 0.9702, Val Loss: 0.5166, Val Acc: 0.8930, Val F1: 0.8911\n",
            "Seed 8 | test_loss: 0.4868 | test_acc: 0.8975 | AUC: 0.9648 | Precision: 0.8985 | Recall: 0.8975 | F1: 0.8968\n",
            "\n",
            "===== SEED 10: 9 =====\n",
            "N=14530, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.7325, Val Loss: 0.8243, Val Acc: 0.7783, Val F1: 0.7142\n",
            "Epoch 2/20 - Train Acc: 0.7784, Val Loss: 0.6478, Val Acc: 0.8173, Val F1: 0.7725\n",
            "Epoch 3/20 - Train Acc: 0.8294, Val Loss: 0.5495, Val Acc: 0.8326, Val F1: 0.8164\n",
            "Epoch 4/20 - Train Acc: 0.8583, Val Loss: 0.4795, Val Acc: 0.8647, Val F1: 0.8419\n",
            "Epoch 5/20 - Train Acc: 0.8728, Val Loss: 0.4372, Val Acc: 0.8784, Val F1: 0.8688\n",
            "Epoch 6/20 - Train Acc: 0.8819, Val Loss: 0.4899, Val Acc: 0.8693, Val F1: 0.8547\n",
            "Epoch 7/20 - Train Acc: 0.8919, Val Loss: 0.4230, Val Acc: 0.8853, Val F1: 0.8787\n",
            "Epoch 8/20 - Train Acc: 0.8970, Val Loss: 0.4191, Val Acc: 0.8807, Val F1: 0.8721\n",
            "Epoch 9/20 - Train Acc: 0.9043, Val Loss: 0.4425, Val Acc: 0.8830, Val F1: 0.8754\n",
            "Epoch 10/20 - Train Acc: 0.9089, Val Loss: 0.4271, Val Acc: 0.8754, Val F1: 0.8723\n",
            "Epoch 11/20 - Train Acc: 0.9129, Val Loss: 0.4590, Val Acc: 0.8739, Val F1: 0.8725\n",
            "Epoch 12/20 - Train Acc: 0.9315, Val Loss: 0.4293, Val Acc: 0.8945, Val F1: 0.8890\n",
            "Epoch 13/20 - Train Acc: 0.9384, Val Loss: 0.4618, Val Acc: 0.8838, Val F1: 0.8795\n",
            "Epoch 14/20 - Train Acc: 0.9405, Val Loss: 0.4738, Val Acc: 0.8708, Val F1: 0.8687\n",
            "Epoch 15/20 - Train Acc: 0.9544, Val Loss: 0.4812, Val Acc: 0.8891, Val F1: 0.8870\n",
            "Epoch 16/20 - Train Acc: 0.9586, Val Loss: 0.5128, Val Acc: 0.8884, Val F1: 0.8863\n",
            "Epoch 17/20 - Train Acc: 0.9620, Val Loss: 0.5356, Val Acc: 0.8815, Val F1: 0.8817\n",
            "Epoch 18/20 - Train Acc: 0.9679, Val Loss: 0.5306, Val Acc: 0.8891, Val F1: 0.8853\n",
            "Epoch 19/20 - Train Acc: 0.9735, Val Loss: 0.5663, Val Acc: 0.8838, Val F1: 0.8816\n",
            "Epoch 20/20 - Train Acc: 0.9719, Val Loss: 0.5749, Val Acc: 0.8807, Val F1: 0.8797\n",
            "Seed 9 | test_loss: 0.4844 | test_acc: 0.8940 | AUC: 0.9628 | Precision: 0.8956 | Recall: 0.8940 | F1: 0.8934\n",
            "\n",
            "===== SUMMARY ACROSS 10 SEEDS =====\n",
            "Average Test Accuracy: 0.8919 ± 0.0168\n",
            "Average Test AUC: 0.9615 ± 0.0062\n",
            "Average Test Precision: 0.8906 ± 0.0108\n",
            "Average Test Recall: 0.8919 ± 0.0168\n",
            "Average Test F1: 0.8897 ± 0.0148\n"
          ]
        }
      ],
      "source": [
        "from torch import optim\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = int(torch.max(y).item() + 1)\n",
        "all_results = []\n",
        "\n",
        "\n",
        "seeds = list(range(10))\n",
        "\n",
        "for seed_idx, seed in enumerate(seeds, 1):\n",
        "    print(f\"\\n===== SEED {seed_idx}: {seed} =====\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    assert N == y.shape[0], \"X and y must have same first dimension\"\n",
        "    num_classes = int(torch.max(y).item() + 1)\n",
        "    print(f\"N={N}, num_classes={num_classes}, device={device}\")\n",
        "\n",
        "    all_indices = list(range(len(X)))\n",
        "    all_labels = y.numpy()\n",
        "\n",
        "    trainval_indices, test_indices = train_test_split(\n",
        "        all_indices,\n",
        "        test_size=0.1,\n",
        "        random_state=seed,\n",
        "        stratify=all_labels\n",
        "    )\n",
        "\n",
        "    trainval_labels = [all_labels[i] for i in trainval_indices]\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        trainval_indices,\n",
        "        test_size=0.1,\n",
        "        random_state=seed,\n",
        "        stratify=trainval_labels\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = TensorDataset(X[train_indices], y[train_indices])\n",
        "    val_ds = TensorDataset(X[val_indices], y[val_indices])\n",
        "    test_ds = TensorDataset(X[test_indices], y[test_indices])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ECGModel(sequence_length=X.shape[1], num_channels=X.shape[2],\n",
        "                     d_model=128, num_heads=4, dff=128, dropout_rate=0.2,\n",
        "                     num_classes=num_classes, apply_softmax=False).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        seen = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            seen += xb.size(0)\n",
        "\n",
        "        train_acc = running_correct / seen\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc, val_precision, val_recall, val_auc, val_f1 = evaluate_on_loader(\n",
        "            model, val_loader, criterion, device, num_classes\n",
        "        )\n",
        "        val_acc_list.append(val_acc)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Test evaluation after final epoch using updated function\n",
        "    test_loss, test_acc, test_precision, test_recall, test_auc, test_f1 = evaluate_on_loader(\n",
        "        model, test_loader, criterion, device, num_classes\n",
        "    )\n",
        "\n",
        "    print(f\"Seed {seed} | test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | \"\n",
        "          f\"AUC: {test_auc:.4f} | Precision: {test_precision:.4f} | \"\n",
        "          f\"Recall: {test_recall:.4f} | F1: {test_f1:.4f}\")\n",
        "\n",
        "    all_results.append({\n",
        "        'seed': seed,\n",
        "        'train_acc_list': train_acc_list,\n",
        "        'val_acc_list': val_acc_list,\n",
        "        'test_acc': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'test_auc': test_auc,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'test_f1': test_f1\n",
        "    })\n",
        "\n",
        "# Summary across seeds\n",
        "test_accs = [r['test_acc'] for r in all_results]\n",
        "test_aucs = [r['test_auc'] for r in all_results if not np.isnan(r['test_auc'])]\n",
        "test_precisions = [r['test_precision'] for r in all_results]\n",
        "test_recalls = [r['test_recall'] for r in all_results]\n",
        "test_f1s = [r['test_f1'] for r in all_results]\n",
        "\n",
        "print(f\"\\n===== SUMMARY ACROSS {len(seeds)} SEEDS =====\")\n",
        "print(f\"Average Test Accuracy: {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
        "if test_aucs:\n",
        "    print(f\"Average Test AUC: {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n",
        "else:\n",
        "    print(f\"Average Test AUC: Could not be calculated\")\n",
        "print(f\"Average Test Precision: {np.mean(test_precisions):.4f} ± {np.std(test_precisions):.4f}\")\n",
        "print(f\"Average Test Recall: {np.mean(test_recalls):.4f} ± {np.std(test_recalls):.4f}\")\n",
        "print(f\"Average Test F1: {np.mean(test_f1s):.4f} ± {np.std(test_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPY7O7VErd6r"
      },
      "outputs": [],
      "source": [
        "!pip install thop calflops -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkQV3Le9tgZ9",
        "outputId": "ca87795a-38d6-4cae-edfe-4fb94ec46dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------- Calculate Flops Results -------------------------------------\n",
            "Notations:\n",
            "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
            "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
            "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
            "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
            "\n",
            "Total Training Params:                                                  10.65 M \n",
            "fwd MACs:                                                               373.133 MMACs\n",
            "fwd FLOPs:                                                              751.428 MFLOPS\n",
            "fwd+bwd MACs:                                                           1.1194 GMACs\n",
            "fwd+bwd FLOPs:                                                          2.2543 GFLOPS\n",
            "\n",
            "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
            "Each module caculated is listed after its name in the following order: \n",
            "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
            "\n",
            "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
            " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
            "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
            "\n",
            "ECGModel(\n",
            "  10.65 M = 100% Params, 373.13 MMACs = 100% MACs, 751.43 MFLOPS = 100% FLOPs\n",
            "  (conv1): Conv1d(8.1 K = 0.076% Params, 40.32 MMACs = 10.8058% MACs, 80.8 MFLOPS = 10.7529% FLOPs, 12, 32, kernel_size=(21,), stride=(1,), padding=(10,))\n",
            "  (bn1): BatchNorm1d(64 = 0.0006% Params, 0 MACs = 0% MACs, 320 KFLOPS = 0.0426% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (ca1): ChannelAttention(\n",
            "    292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 321.03 KFLOPS = 0.0427% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 1.03 KFLOPS = 0.0001% FLOPs\n",
            "      (0): Linear(132 = 0.0012% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=32, out_features=4, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 8 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(160 = 0.0015% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=4, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool1): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 160 KFLOPS = 0.0213% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv1d(23.58 K = 0.2214% Params, 58.88 MMACs = 15.7799% MACs, 117.84 MFLOPS = 15.6821% FLOPs, 32, 32, kernel_size=(23,), stride=(1,), padding=(11,))\n",
            "  (bn2): BatchNorm1d(64 = 0.0006% Params, 0 MACs = 0% MACs, 160 KFLOPS = 0.0213% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (ca2): ChannelAttention(\n",
            "    292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 161.03 KFLOPS = 0.0214% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 1.03 KFLOPS = 0.0001% FLOPs\n",
            "      (0): Linear(132 = 0.0012% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=32, out_features=4, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 8 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(160 = 0.0015% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=4, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool2): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0106% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv1d(51.26 K = 0.4813% Params, 64 MMACs = 17.1521% MACs, 128.08 MFLOPS = 17.0449% FLOPs, 32, 64, kernel_size=(25,), stride=(1,), padding=(12,))\n",
            "  (bn3): BatchNorm1d(128 = 0.0012% Params, 0 MACs = 0% MACs, 160 KFLOPS = 0.0213% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (ca3): ChannelAttention(\n",
            "    1.1 K = 0.0103% Params, 2.05 KMACs = 0.0005% MACs, 164.11 KFLOPS = 0.0218% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      1.1 K = 0.0103% Params, 2.05 KMACs = 0.0005% MACs, 4.11 KFLOPS = 0.0005% FLOPs\n",
            "      (0): Linear(520 = 0.0049% Params, 1.02 KMACs = 0.0003% MACs, 2.05 KFLOPS = 0.0003% FLOPs, in_features=64, out_features=8, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 16 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(576 = 0.0054% Params, 1.02 KMACs = 0.0003% MACs, 2.05 KFLOPS = 0.0003% FLOPs, in_features=8, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool3): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0106% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv1d(221.31 K = 2.0777% Params, 138.24 MMACs = 37.0485% MACs, 276.56 MFLOPS = 36.8046% FLOPs, 64, 128, kernel_size=(27,), stride=(1,), padding=(13,))\n",
            "  (bn4): BatchNorm1d(256 = 0.0024% Params, 0 MACs = 0% MACs, 160 KFLOPS = 0.0213% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (ca4): ChannelAttention(\n",
            "    4.24 K = 0.0398% Params, 8.19 KMACs = 0.0022% MACs, 176.42 KFLOPS = 0.0235% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      4.24 K = 0.0398% Params, 8.19 KMACs = 0.0022% MACs, 16.42 KFLOPS = 0.0022% FLOPs\n",
            "      (0): Linear(2.06 K = 0.0194% Params, 4.1 KMACs = 0.0011% MACs, 8.19 KFLOPS = 0.0011% FLOPs, in_features=128, out_features=16, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(2.18 K = 0.0204% Params, 4.1 KMACs = 0.0011% MACs, 8.19 KFLOPS = 0.0011% FLOPs, in_features=16, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (transformer_block): TransformerEncoderBlock(\n",
            "    99.58 K = 0.9349% Params, 61.44 MMACs = 16.466% MACs, 125.32 MFLOPS = 16.6779% FLOPs\n",
            "    (mha): MultiheadAttention(\n",
            "      66.05 K = 0.6201% Params, 40.96 MMACs = 10.9773% MACs, 83.48 MFLOPS = 11.1098% FLOPs\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(16.51 K = 0.155% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (dropout1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
            "    (norm1): LayerNorm(256 = 0.0024% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0532% FLOPs, (128,), eps=1e-06, elementwise_affine=True)\n",
            "    (ffn): Sequential(\n",
            "      33.02 K = 0.31% Params, 20.48 MMACs = 5.4887% MACs, 41.04 MFLOPS = 5.4616% FLOPs\n",
            "      (0): Linear(16.51 K = 0.155% Params, 10.24 MMACs = 2.7443% MACs, 20.48 MFLOPS = 2.7255% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0106% FLOPs, inplace=True)\n",
            "      (2): Linear(16.51 K = 0.155% Params, 10.24 MMACs = 2.7443% MACs, 20.48 MFLOPS = 2.7255% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (dropout2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
            "    (norm2): LayerNorm(256 = 0.0024% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0532% FLOPs, (128,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (flatten): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
            "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
            "  (fc_out): Linear(1.42 K = 0.0133% Params, 1.41 KMACs = 0.0004% MACs, 2.82 KFLOPS = 0.0004% FLOPs, in_features=128, out_features=11, bias=True)\n",
            "  (fc1): Linear(10.24 M = 96.135% Params, 10.24 MMACs = 2.7443% MACs, 20.48 MFLOPS = 2.7255% FLOPs, in_features=80000, out_features=128, bias=True)\n",
            ")\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model FLOPs:751.428 MFLOPS   MACs:373.133 MMACs   Params:10.6518 M \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import calflops\n",
        "\n",
        "flops, macs, params = calflops.calculate_flops(model=model,\n",
        "                                      input_shape=(1, 5000, 12),\n",
        "                                      output_as_string=True,\n",
        "                                      output_precision=4)\n",
        "print(\"Model FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krc4M5XWthio",
        "outputId": "88efb33d-05e2-4c70-a082-ec3afeb9cda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL COMPLEXITY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "📊 Parameter Count:\n",
            "   Total Parameters:       10,651,819\n",
            "   Trainable Parameters:   10,651,819\n",
            "   Non-trainable Parameters: 0\n",
            "   Model Size (MB):        40.63\n",
            "\n",
            "🔢 FLOPs (Floating Point Operations):\n",
            "   MACs (Multiply-Accumulate): 334.413M\n",
            "   Parameters (thop):          10.586M\n",
            "\n",
            "📋 Layer-wise Parameter Breakdown:\n",
            "Layer Name                                    Parameters   % of Total\n",
            "----------------------------------------------------------------------\n",
            "conv1.weight                                       8,064        0.08%\n",
            "conv1.bias                                            32        0.00%\n",
            "bn1.weight                                            32        0.00%\n",
            "bn1.bias                                              32        0.00%\n",
            "ca1.mlp.0.weight                                     128        0.00%\n",
            "ca1.mlp.0.bias                                         4        0.00%\n",
            "ca1.mlp.2.weight                                     128        0.00%\n",
            "ca1.mlp.2.bias                                        32        0.00%\n",
            "conv2.weight                                      23,552        0.22%\n",
            "conv2.bias                                            32        0.00%\n",
            "bn2.weight                                            32        0.00%\n",
            "bn2.bias                                              32        0.00%\n",
            "ca2.mlp.0.weight                                     128        0.00%\n",
            "ca2.mlp.0.bias                                         4        0.00%\n",
            "ca2.mlp.2.weight                                     128        0.00%\n",
            "ca2.mlp.2.bias                                        32        0.00%\n",
            "conv3.weight                                      51,200        0.48%\n",
            "conv3.bias                                            64        0.00%\n",
            "bn3.weight                                            64        0.00%\n",
            "bn3.bias                                              64        0.00%\n",
            "ca3.mlp.0.weight                                     512        0.00%\n",
            "ca3.mlp.0.bias                                         8        0.00%\n",
            "ca3.mlp.2.weight                                     512        0.00%\n",
            "ca3.mlp.2.bias                                        64        0.00%\n",
            "conv4.weight                                     221,184        2.08%\n",
            "conv4.bias                                           128        0.00%\n",
            "bn4.weight                                           128        0.00%\n",
            "bn4.bias                                             128        0.00%\n",
            "ca4.mlp.0.weight                                   2,048        0.02%\n",
            "ca4.mlp.0.bias                                        16        0.00%\n",
            "ca4.mlp.2.weight                                   2,048        0.02%\n",
            "ca4.mlp.2.bias                                       128        0.00%\n",
            "transformer_block.mha.in_proj_weight              49,152        0.46%\n",
            "transformer_block.mha.in_proj_bias                   384        0.00%\n",
            "transformer_block.mha.out_proj.weight             16,384        0.15%\n",
            "transformer_block.mha.out_proj.bias                  128        0.00%\n",
            "transformer_block.norm1.weight                       128        0.00%\n",
            "transformer_block.norm1.bias                         128        0.00%\n",
            "transformer_block.ffn.0.weight                    16,384        0.15%\n",
            "transformer_block.ffn.0.bias                         128        0.00%\n",
            "transformer_block.ffn.2.weight                    16,384        0.15%\n",
            "transformer_block.ffn.2.bias                         128        0.00%\n",
            "transformer_block.norm2.weight                       128        0.00%\n",
            "transformer_block.norm2.bias                         128        0.00%\n",
            "fc_out.weight                                      1,408        0.01%\n",
            "fc_out.bias                                           11        0.00%\n",
            "fc1.weight                                    10,240,000       96.13%\n",
            "fc1.bias                                             128        0.00%\n",
            "\n",
            "📝 Detailed Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'total_params': 10651819,\n",
              " 'trainable_params': 10651819,\n",
              " 'model_size_mb': 40.63346481323242}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "from thop import profile, clever_format\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "def analyze_model_complexity(model, input_size=(1, 5000, 12), device='cuda'):\n",
        "    \"\"\"\n",
        "    Analyze model complexity: parameters, FLOPs, memory\n",
        "\n",
        "    Args:\n",
        "        model: Your HANWithAttention model\n",
        "        input_size: (batch, segments, timesteps, channels)\n",
        "        device: 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODEL COMPLEXITY ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable_params = total_params - trainable_params\n",
        "\n",
        "    print(f\"\\nParameter Count:\")\n",
        "    print(f\"   Total Parameters:       {total_params:,}\")\n",
        "    print(f\"   Trainable Parameters:   {trainable_params:,}\")\n",
        "    print(f\"   Non-trainable Parameters: {non_trainable_params:,}\")\n",
        "    print(f\"   Model Size (MB):        {total_params * 4 / (1024**2):.2f}\")  # 4 bytes per float32\n",
        "\n",
        "    # 2. FLOPs calculation\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "\n",
        "    try:\n",
        "        macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "        macs, params = clever_format([macs, params], \"%.3f\")\n",
        "        print(f\"\\nFLOPs (Floating Point Operations):\")\n",
        "        print(f\"   MACs (Multiply-Accumulate): {macs}\")\n",
        "        print(f\"   Parameters (thop):          {params}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFLOPs calculation failed: {e}\")\n",
        "\n",
        "    # 3. Layer-wise parameter breakdown\n",
        "    print(f\"\\nLayer-wise Parameter Breakdown:\")\n",
        "    print(f\"{'Layer Name':<40} {'Parameters':>15} {'% of Total':>12}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            param_count = param.numel()\n",
        "            percentage = 100 * param_count / trainable_params\n",
        "            print(f\"{name:<40} {param_count:>15,} {percentage:>11.2f}%\")\n",
        "\n",
        "    # 4. Detailed model summary\n",
        "    print(f\"\\nDetailed Model Architecture:\")\n",
        "    summary(model,\n",
        "            input_size=input_size,\n",
        "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
        "            depth=4,\n",
        "            device=device,\n",
        "            verbose=0)\n",
        "\n",
        "    return {\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'model_size_mb': total_params * 4 / (1024**2)\n",
        "    }\n",
        "\n",
        "analyze_model_complexity(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI1SVcJvtjMK",
        "outputId": "759ac99a-8c61-477b-a5d3-7c7131df1ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model size: 40.636MB\n"
          ]
        }
      ],
      "source": [
        "# From: https://discuss.pytorch.org/t/finding-model-size/130275\n",
        "\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYFGsjptleX",
        "outputId": "50fe6cf1-341d-439e-f312-55da29da8ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INFERENCE BENCHMARK\n",
            "================================================================================\n",
            "Device: cpu\n",
            "Input shape: (1, 5000, 12)\n",
            "Throughput batch size: 16\n",
            "\n",
            "Batch-1 Latency (ms):\n",
            "     mean_ms: 7.674\n",
            "      std_ms: 0.426\n",
            "   median_ms: 7.567\n",
            "      min_ms: 7.402\n",
            "      max_ms: 10.701\n",
            "\n",
            "Batch-N Throughput:\n",
            "     batch_time_ms: 78.613\n",
            "     per_sample_ms: 4.913\n",
            "   samples_per_sec: 203.53\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark Environment Setup\n",
        "# =============================================================================\n",
        "\n",
        "def setup_benchmark_env():\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = False\n",
        "    torch.backends.cudnn.allow_tf32 = False\n",
        "\n",
        "def time_forward(model, inputs, device='cuda'):\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "        _ = model(inputs)\n",
        "        end.record()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        return start.elapsed_time(end) / 1000.0  # seconds\n",
        "    else:\n",
        "        t0 = time.time()\n",
        "        _ = model(inputs)\n",
        "        return time.time() - t0\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Batch-1 Latency Benchmark\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark_latency(\n",
        "    model,\n",
        "    input_shape,\n",
        "    device='cuda',\n",
        "    runs=100,\n",
        "    warmup=20\n",
        "):\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            x = torch.randn(input_shape, device=device)\n",
        "            _ = model(x)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        for _ in range(runs):\n",
        "            x = torch.randn(input_shape, device=device)\n",
        "            elapsed = time_forward(model, x, device)\n",
        "            times.append(elapsed)\n",
        "\n",
        "    times = np.array(times)\n",
        "\n",
        "    return {\n",
        "        \"mean_ms\": times.mean() * 1000,\n",
        "        \"std_ms\": times.std() * 1000,\n",
        "        \"median_ms\": np.median(times) * 1000,\n",
        "        \"min_ms\": times.min() * 1000,\n",
        "        \"max_ms\": times.max() * 1000,\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Batch-N Throughput Benchmark\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark_throughput(\n",
        "    model,\n",
        "    input_shape,\n",
        "    batch_size,\n",
        "    device='cuda',\n",
        "    runs=100,\n",
        "    warmup=20\n",
        "):\n",
        "    model = model.to(device).eval()\n",
        "    shape = (batch_size,) + input_shape[1:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            x = torch.randn(shape, device=device)\n",
        "            _ = model(x)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        for _ in range(runs):\n",
        "            x = torch.randn(shape, device=device)\n",
        "            elapsed = time_forward(model, x, device)\n",
        "            times.append(elapsed)\n",
        "\n",
        "    times = np.array(times)\n",
        "\n",
        "    return {\n",
        "        \"batch_time_ms\": times.mean() * 1000,\n",
        "        \"per_sample_ms\": (times.mean() / batch_size) * 1000,\n",
        "        \"samples_per_sec\": batch_size / times.mean()\n",
        "    }\n",
        "\n",
        "def run_inference_benchmark(\n",
        "    model,\n",
        "    input_shape=(1, 5000, 12),\n",
        "    batch_size=16,\n",
        "    device='cuda'\n",
        "):\n",
        "    setup_benchmark_env()\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"INFERENCE BENCHMARK\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Input shape: {input_shape}\")\n",
        "    print(f\"Throughput batch size: {batch_size}\")\n",
        "\n",
        "    latency = benchmark_latency(\n",
        "        model=model,\n",
        "        input_shape=input_shape,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    throughput = benchmark_throughput(\n",
        "        model=model,\n",
        "        input_shape=input_shape,\n",
        "        batch_size=batch_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"\\nBatch-1 Latency (ms):\")\n",
        "    for k, v in latency.items():\n",
        "        print(f\"  {k:>10}: {v:.3f}\")\n",
        "\n",
        "    print(\"\\nBatch-N Throughput:\")\n",
        "    for k, v in throughput.items():\n",
        "        if \"ms\" in k:\n",
        "            print(f\"  {k:>16}: {v:.3f}\")\n",
        "        else:\n",
        "            print(f\"  {k:>16}: {v:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"latency\": latency,\n",
        "        \"throughput\": throughput\n",
        "    }\n",
        "\n",
        "results = run_inference_benchmark(\n",
        "    model,\n",
        "    input_shape=(1, 5000, 12),\n",
        "    batch_size=16,\n",
        "    device='cpu'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxcSskO7vKbA",
        "outputId": "a31fcce5-2b20-448e-d3b4-95c00b1ed8d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INFERENCE BENCHMARK\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Input shape: (1, 5000, 12)\n",
            "Throughput batch size: 16\n",
            "\n",
            "Batch-1 Latency (ms):\n",
            "     mean_ms: 2.933\n",
            "      std_ms: 0.107\n",
            "   median_ms: 2.903\n",
            "      min_ms: 2.858\n",
            "      max_ms: 3.589\n",
            "\n",
            "Batch-N Throughput:\n",
            "     batch_time_ms: 5.829\n",
            "     per_sample_ms: 0.364\n",
            "   samples_per_sec: 2744.69\n"
          ]
        }
      ],
      "source": [
        "results = run_inference_benchmark(\n",
        "    model,\n",
        "    input_shape=(1, 5000, 12),\n",
        "    batch_size=16,\n",
        "    device='cuda'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
