{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA6ZO5hfFjPJ",
        "outputId": "2d5aba45-1d16-4628-a356-ca4649cb2699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q wfdb torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAlJDPvFFpjp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Optional\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset\n",
        "import time\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from scipy.signal import butter, filtfilt, find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN-Kxa3MGdTI",
        "outputId": "56189d76-d3fd-4076-f989-025269c86980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Successfully extracted /content/drive/MyDrive/records.zip to /content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/records.zip'\n",
        "\n",
        "extract_path = '/content'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Check if the zip file exists\n",
        "if os.path.exists(zip_file_path):\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Successfully extracted {zip_file_path} to {extract_path}\")\n",
        "else:\n",
        "    print(f\"Error: {zip_file_path} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmV9C6W9GmU_"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# utilities: denoising\n",
        "# -------------------------\n",
        "def denoise(data):\n",
        "    # wavelet transform\n",
        "    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)\n",
        "    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
        "\n",
        "    # Threshold denoising\n",
        "    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))\n",
        "    cD1.fill(0)\n",
        "    cD2.fill(0)\n",
        "    for i in range(1, len(coeffs) - 2):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
        "\n",
        "    # Inverse wavelet transform to obtain the denoised signal\n",
        "    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')\n",
        "    return rdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alEPxMxcJaF6"
      },
      "outputs": [],
      "source": [
        "PATH = \"records\"\n",
        "\n",
        "# Lead order is I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
        "SAMPLING_RATE = 500\n",
        "PRE_PEAK_SAMPLES = 99\n",
        "POST_PEAK_SAMPLES = 201\n",
        "SEGMENT_LENGTH = 300\n",
        "\n",
        "metadata = pd.read_csv(\"metadata.csv\")\n",
        "codes = pd.read_csv(\"code.csv\")\n",
        "\n",
        "# Get the code -> mappnigs\n",
        "code_mappings = codes[['Code', 'Category']]\n",
        "\n",
        "# Convert to a dict\n",
        "code_mappings = {str(key): value for (_, (key, value)) in code_mappings.iterrows()}\n",
        "\n",
        "# Removes samples with more than 1 class\n",
        "metadata = metadata[~metadata['AHA_Code'].str.contains(\";\")]\n",
        "\n",
        "# Remove modifiers\n",
        "metadata['AHA_Code'] = [code.split(\"+\")[0] for code in metadata['AHA_Code']]\n",
        "\n",
        "# Map AHA code to category\n",
        "metadata['AHA_Code_Mapped'] = metadata['AHA_Code'].map(code_mappings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5FA2fRXvx1j"
      },
      "outputs": [],
      "source": [
        "# metadata = metadata[~metadata['AHA_Code_Mapped'].isin(['M', 'H' 'K'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "j-Lss-kMM94d",
        "outputId": "eed053b2-b0f2-4619-9b1f-31308d31155e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AHA_Code_Mapped</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>13903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L</th>\n",
              "      <td>2713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>2575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>1355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J</th>\n",
              "      <td>350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D</th>\n",
              "      <td>261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>K</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "AHA_Code_Mapped\n",
              "A    13903\n",
              "L     2713\n",
              "C     2575\n",
              "I     1355\n",
              "F      506\n",
              "J      350\n",
              "E      266\n",
              "D      261\n",
              "M       56\n",
              "H       35\n",
              "K       26\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata['AHA_Code_Mapped'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfDVlnW2Gs1W",
        "outputId": "454f302c-9ddc-4110-8b9d-53c7b844d608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 22046\n",
            "Y shape (one-hot): (22046, 11)\n"
          ]
        }
      ],
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for data in metadata.itertuples():\n",
        "    ecg_data_path = os.path.join(PATH, data.ECG_ID + \".h5\")\n",
        "\n",
        "    ecg_data = h5py.File(ecg_data_path)\n",
        "\n",
        "    ecg = ecg_data['ecg']\n",
        "\n",
        "    ecg = ecg[:, :5000]\n",
        "\n",
        "    denoised_ecg = np.array([denoise(lead) for lead in ecg])\n",
        "\n",
        "    X.append(np.array(denoised_ecg))\n",
        "    Y.append(data.AHA_Code_Mapped)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(Y)\n",
        "y = pd.get_dummies(Y, dtype=int).to_numpy()\n",
        "\n",
        "print(\"Number of samples:\", len(X))\n",
        "print(\"Y shape (one-hot):\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWdEc4uChbvT",
        "outputId": "c84971af-5429-4f96-9b32-44ad9024203a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(22046, 5000, 12)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = X.transpose(0, 2, 1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Pgck299WZX"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(np.array(y).argmax(axis=1), dtype=torch.long)  # (14530,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76xZ_p8601Es",
        "outputId": "eaf8c522-1491-479b-db3a-e7b27da0d053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output shape: torch.Size([2, 11])\n"
          ]
        }
      ],
      "source": [
        "# pytorch_ecg_model.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Channel Attention (CBAM)\n",
        "# -------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, ratio=8):\n",
        "        super().__init__()\n",
        "        hidden = max(1, channels // ratio)\n",
        "        # Shared MLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(channels, hidden, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, channels, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        b, c, l = x.shape\n",
        "\n",
        "        avg_pool = F.adaptive_avg_pool1d(x, 1).view(b, c)   # (B, C)\n",
        "        max_pool = F.adaptive_max_pool1d(x, 1).view(b, c)   # (B, C)\n",
        "        avg_out = self.mlp(avg_pool)  # (B, C)\n",
        "        max_out = self.mlp(max_pool)  # (B, C)\n",
        "\n",
        "        out = torch.sigmoid(avg_out + max_out).unsqueeze(-1)  # (B, C, 1)\n",
        "        return x * out  # broadcasting over length\n",
        "\n",
        "# -------------------------\n",
        "# Spatial Attention (CBAM)\n",
        "# -------------------------\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv1d(in_channels=2, out_channels=1,\n",
        "                              kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        # we will apply sigmoid after conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        # channel-wise avg and max -> (B, 1, L)\n",
        "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        cat = torch.cat([avg_pool, max_pool], dim=1)  # (B, 2, L)\n",
        "        attn = torch.sigmoid(self.conv(cat))  # (B, 1, L)\n",
        "        return x * attn  # broadcast over channels\n",
        "\n",
        "# -------------------------\n",
        "# Transformer Encoder Block\n",
        "# -------------------------\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        # Use batch_first=True so inputs are (B, L, D)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads,\n",
        "                                         dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, x, src_mask=None, src_key_padding_mask=None):\n",
        "        # x: (B, L, D)\n",
        "        attn_out, _ = self.mha(x, x, x,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)\n",
        "        attn_out = self.dropout1(attn_out)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        ffn_out = self.dropout2(ffn_out)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x  # (B, L, D)\n",
        "\n",
        "# -------------------------\n",
        "# Positional Encoding (sinusoidal)\n",
        "# -------------------------\n",
        "def sinusoidal_positional_encoding(seq_len, d_model, device=None, dtype=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cpu')\n",
        "    pe = torch.zeros(seq_len, d_model, device=device, dtype=dtype)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device, dtype=torch.float)\n",
        "                         * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "\n",
        "# -------------------------\n",
        "# Full Model\n",
        "# -------------------------\n",
        "class ECGModel(nn.Module):\n",
        "    def __init__(self, sequence_length=5000, num_channels=12,\n",
        "                 d_model=128, num_heads=4, dff=128, dropout_rate=0.2,\n",
        "                 num_classes=11, apply_softmax=False):\n",
        "        super().__init__()\n",
        "        self.apply_softmax = apply_softmax\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=32, kernel_size=21, padding=(21-1)//2)\n",
        "        # self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.bn1 = nn.GroupNorm(num_groups=8, num_channels=32)\n",
        "        self.ca1 = ChannelAttention(32, ratio=8)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  # approx 'same' behaviour\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 32, kernel_size=23, padding=(23-1)//2)\n",
        "        # self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.bn2 = nn.GroupNorm(num_groups=8, num_channels=32)\n",
        "        self.ca2 = ChannelAttention(32, ratio=8)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(32, 64, kernel_size=25, padding=(25-1)//2)\n",
        "        # self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.bn3 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
        "        self.ca3 = ChannelAttention(64, ratio=8)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv4 = nn.Conv1d(64, 128, kernel_size=27, padding=(27-1)//2)\n",
        "        # self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.bn4 = nn.GroupNorm(num_groups=8, num_channels=128)\n",
        "        self.ca4 = ChannelAttention(128, ratio=8)\n",
        "\n",
        "        if 128 != d_model:\n",
        "            self.project_to_d_model = nn.Conv1d(128, d_model, kernel_size=1)\n",
        "        else:\n",
        "            self.project_to_d_model = None\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.transformer_block = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads,\n",
        "                                                         dff=dff, dropout_rate=dropout_rate)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = None \n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc_out = nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "        self._configured = False\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _init_fc_head(self, sample_batch, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            x = sample_batch.to(device)\n",
        "            features = self._forward_features(x)  # returns (B, L, D) after transformer\n",
        "            flat = features.reshape(features.size(0), -1)\n",
        "            in_features = flat.size(1)\n",
        "            self.fc1 = nn.Linear(in_features, 128).to(device)\n",
        "            nn.init.xavier_uniform_(self.fc1.weight)\n",
        "            nn.init.zeros_(self.fc1.bias)\n",
        "            self._configured = True\n",
        "\n",
        "\n",
        "    def _forward_features(self, x):\n",
        "        x = x.permute(0, 2, 1)  # -> (B, C, L)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.ca4(x)\n",
        "        # x: (B, channels, L_final)\n",
        "\n",
        "        if self.project_to_d_model is not None:\n",
        "            x = self.project_to_d_model(x)  # (B, d_model, L_final)\n",
        "\n",
        "        # Transpose for transformer: (B, L, D)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        seq_len = x.size(1)\n",
        "        pe = sinusoidal_positional_encoding(seq_len, self.d_model, device=x.device, dtype=x.dtype)\n",
        "        x = x + pe  # (B, L, D)\n",
        "\n",
        "        x = self.transformer_block(x)  # (B, L, D)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "\n",
        "        if not self._configured:\n",
        "            self._init_fc_head(x, device)\n",
        "\n",
        "        features = self._forward_features(x)  # (B, L, D)\n",
        "        pooled = features.mean(dim=1)         # (B, D)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.fc_out(pooled)\n",
        "\n",
        "        if self.apply_softmax:\n",
        "            return F.softmax(logits, dim=-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "sequence_length = 5000\n",
        "num_channels = 12\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGModel(sequence_length=sequence_length, num_channels=num_channels,\n",
        "                  d_model=128, num_heads=4, dff=128, dropout_rate=0.3,\n",
        "                  num_classes=11, apply_softmax=False).to(device)\n",
        "\n",
        "dummy = torch.randn(2, sequence_length, num_channels, device=device)\n",
        "out = model(dummy)  # out shape: (2, 11) logits\n",
        "print(\"output shape:\", out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMSLKQ4HVM5_",
        "outputId": "5194299b-cd72-456d-9f39-184eaaf84afb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ECGModel                                 [2, 11]                   10,240,128\n",
              "├─Conv1d: 1-1                            [2, 32, 5000]             8,096\n",
              "├─GroupNorm: 1-2                         [2, 32, 5000]             64\n",
              "├─ChannelAttention: 1-3                  [2, 32, 5000]             --\n",
              "│    └─Sequential: 2-1                   [2, 32]                   --\n",
              "│    │    └─Linear: 3-1                  [2, 4]                    132\n",
              "│    │    └─ReLU: 3-2                    [2, 4]                    --\n",
              "│    │    └─Linear: 3-3                  [2, 32]                   160\n",
              "│    └─Sequential: 2-2                   [2, 32]                   (recursive)\n",
              "│    │    └─Linear: 3-4                  [2, 4]                    (recursive)\n",
              "│    │    └─ReLU: 3-5                    [2, 4]                    --\n",
              "│    │    └─Linear: 3-6                  [2, 32]                   (recursive)\n",
              "├─MaxPool1d: 1-4                         [2, 32, 2500]             --\n",
              "├─Conv1d: 1-5                            [2, 32, 2500]             23,584\n",
              "├─GroupNorm: 1-6                         [2, 32, 2500]             64\n",
              "├─ChannelAttention: 1-7                  [2, 32, 2500]             --\n",
              "│    └─Sequential: 2-3                   [2, 32]                   --\n",
              "│    │    └─Linear: 3-7                  [2, 4]                    132\n",
              "│    │    └─ReLU: 3-8                    [2, 4]                    --\n",
              "│    │    └─Linear: 3-9                  [2, 32]                   160\n",
              "│    └─Sequential: 2-4                   [2, 32]                   (recursive)\n",
              "│    │    └─Linear: 3-10                 [2, 4]                    (recursive)\n",
              "│    │    └─ReLU: 3-11                   [2, 4]                    --\n",
              "│    │    └─Linear: 3-12                 [2, 32]                   (recursive)\n",
              "├─MaxPool1d: 1-8                         [2, 32, 1250]             --\n",
              "├─Conv1d: 1-9                            [2, 64, 1250]             51,264\n",
              "├─GroupNorm: 1-10                        [2, 64, 1250]             128\n",
              "├─ChannelAttention: 1-11                 [2, 64, 1250]             --\n",
              "│    └─Sequential: 2-5                   [2, 64]                   --\n",
              "│    │    └─Linear: 3-13                 [2, 8]                    520\n",
              "│    │    └─ReLU: 3-14                   [2, 8]                    --\n",
              "│    │    └─Linear: 3-15                 [2, 64]                   576\n",
              "│    └─Sequential: 2-6                   [2, 64]                   (recursive)\n",
              "│    │    └─Linear: 3-16                 [2, 8]                    (recursive)\n",
              "│    │    └─ReLU: 3-17                   [2, 8]                    --\n",
              "│    │    └─Linear: 3-18                 [2, 64]                   (recursive)\n",
              "├─MaxPool1d: 1-12                        [2, 64, 625]              --\n",
              "├─Conv1d: 1-13                           [2, 128, 625]             221,312\n",
              "├─GroupNorm: 1-14                        [2, 128, 625]             256\n",
              "├─ChannelAttention: 1-15                 [2, 128, 625]             --\n",
              "│    └─Sequential: 2-7                   [2, 128]                  --\n",
              "│    │    └─Linear: 3-19                 [2, 16]                   2,064\n",
              "│    │    └─ReLU: 3-20                   [2, 16]                   --\n",
              "│    │    └─Linear: 3-21                 [2, 128]                  2,176\n",
              "│    └─Sequential: 2-8                   [2, 128]                  (recursive)\n",
              "│    │    └─Linear: 3-22                 [2, 16]                   (recursive)\n",
              "│    │    └─ReLU: 3-23                   [2, 16]                   --\n",
              "│    │    └─Linear: 3-24                 [2, 128]                  (recursive)\n",
              "├─TransformerEncoderBlock: 1-16          [2, 625, 128]             --\n",
              "│    └─MultiheadAttention: 2-9           [2, 625, 128]             66,048\n",
              "│    └─Dropout: 2-10                     [2, 625, 128]             --\n",
              "│    └─LayerNorm: 2-11                   [2, 625, 128]             256\n",
              "│    └─Sequential: 2-12                  [2, 625, 128]             --\n",
              "│    │    └─Linear: 3-25                 [2, 625, 128]             16,512\n",
              "│    │    └─ReLU: 3-26                   [2, 625, 128]             --\n",
              "│    │    └─Linear: 3-27                 [2, 625, 128]             16,512\n",
              "│    └─Dropout: 2-13                     [2, 625, 128]             --\n",
              "│    └─LayerNorm: 2-14                   [2, 625, 128]             256\n",
              "├─Dropout: 1-17                          [2, 128]                  --\n",
              "├─Linear: 1-18                           [2, 11]                   1,419\n",
              "==========================================================================================\n",
              "Total params: 10,651,819\n",
              "Trainable params: 10,651,819\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 603.77\n",
              "==========================================================================================\n",
              "Input size (MB): 0.48\n",
              "Forward/backward pass size (MB): 17.93\n",
              "Params size (MB): 1.38\n",
              "Estimated Total Size (MB): 19.79\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model, input_size=(2, sequence_length, num_channels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJG_K71EYNCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "def evaluate_on_loader(model, loader, criterion, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels, all_preds, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            logits = model(xb)  # (B, C)\n",
        "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "               print(f\"Found NaN/Inf in logits at batch {i}\")\n",
        "               any_nan = True\n",
        "            loss = criterion(logits, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            labels = yb.cpu().numpy()\n",
        "\n",
        "            all_labels.extend(labels)\n",
        "            all_preds.extend(preds)\n",
        "            all_probs.extend(probs)\n",
        "\n",
        "    avg_loss = running_loss / len(all_labels)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "\n",
        "    try:\n",
        "        y_true = np.eye(num_classes)[all_labels]\n",
        "        y_score = np.array(all_probs)\n",
        "\n",
        "        auc_list = []\n",
        "        for i in range(num_classes):\n",
        "            if np.any(y_true[:, i]):  # class i exists\n",
        "                auc_list.append(roc_auc_score(y_true[:, i], y_score[:, i]))\n",
        "        if auc_list:\n",
        "            auc = np.mean(auc_list)\n",
        "        else:\n",
        "            auc = float(\"nan\")\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return avg_loss, acc, precision, recall, auc, f1\n",
        "\n",
        "\n",
        "def confusion_matrix(preds, targets, num_classes):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for p, t in zip(preds, targets):\n",
        "        cm[int(t), int(p)] += 1\n",
        "    return cm\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzfTFUZmld7R",
        "outputId": "1d9a599e-bad2-4038-fe62-844fc452f97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data checks out: No NaNs or Infs found.\n"
          ]
        }
      ],
      "source": [
        "if torch.isnan(X).any() or torch.isinf(X).any():\n",
        "    print(\"WARNING: Data contains NaNs or Infs!\")\n",
        "    # Filter out bad samples\n",
        "    mask = ~torch.isnan(X).reshape(X.shape[0], -1).any(dim=1)\n",
        "    mask &= ~torch.isinf(X).reshape(X.shape[0], -1).any(dim=1)\n",
        "\n",
        "    print(f\"Removing {X.shape[0] - mask.sum()} bad samples...\")\n",
        "    X = X[mask]\n",
        "    y = y[mask]\n",
        "else:\n",
        "    print(\"Data checks out: No NaNs or Infs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFL-WuWL_icG",
        "outputId": "27000e14-1588-4dd1-f7ed-4d536f3328a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== SEED 1: 0 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5728, Val Loss: 1.1120, Val Acc: 0.6564, Val F1: 0.5421\n",
            "Epoch 2/20 - Train Acc: 0.7012, Val Loss: 0.8685, Val Acc: 0.7270, Val F1: 0.6637\n",
            "Epoch 3/20 - Train Acc: 0.7579, Val Loss: 0.7166, Val Acc: 0.7783, Val F1: 0.7583\n",
            "Epoch 4/20 - Train Acc: 0.7923, Val Loss: 0.6588, Val Acc: 0.8025, Val F1: 0.7799\n",
            "Epoch 5/20 - Train Acc: 0.8053, Val Loss: 0.6190, Val Acc: 0.8171, Val F1: 0.7967\n",
            "Epoch 6/20 - Train Acc: 0.8180, Val Loss: 0.5882, Val Acc: 0.8176, Val F1: 0.7989\n",
            "Epoch 7/20 - Train Acc: 0.8271, Val Loss: 0.5548, Val Acc: 0.8242, Val F1: 0.8084\n",
            "Epoch 8/20 - Train Acc: 0.8345, Val Loss: 0.5737, Val Acc: 0.8196, Val F1: 0.8040\n",
            "Epoch 9/20 - Train Acc: 0.8415, Val Loss: 0.5440, Val Acc: 0.8272, Val F1: 0.8191\n",
            "Epoch 10/20 - Train Acc: 0.8488, Val Loss: 0.5368, Val Acc: 0.8343, Val F1: 0.8222\n",
            "Epoch 11/20 - Train Acc: 0.8541, Val Loss: 0.5548, Val Acc: 0.8217, Val F1: 0.8107\n",
            "Epoch 12/20 - Train Acc: 0.8595, Val Loss: 0.5321, Val Acc: 0.8373, Val F1: 0.8264\n",
            "Epoch 13/20 - Train Acc: 0.8702, Val Loss: 0.5219, Val Acc: 0.8438, Val F1: 0.8356\n",
            "Epoch 14/20 - Train Acc: 0.8761, Val Loss: 0.5390, Val Acc: 0.8322, Val F1: 0.8241\n",
            "Epoch 15/20 - Train Acc: 0.8800, Val Loss: 0.5393, Val Acc: 0.8398, Val F1: 0.8300\n",
            "Epoch 16/20 - Train Acc: 0.8847, Val Loss: 0.5472, Val Acc: 0.8413, Val F1: 0.8312\n",
            "Epoch 17/20 - Train Acc: 0.8890, Val Loss: 0.5407, Val Acc: 0.8388, Val F1: 0.8306\n",
            "Epoch 18/20 - Train Acc: 0.8917, Val Loss: 0.5417, Val Acc: 0.8388, Val F1: 0.8303\n",
            "Epoch 19/20 - Train Acc: 0.8950, Val Loss: 0.5438, Val Acc: 0.8403, Val F1: 0.8319\n",
            "Epoch 20/20 - Train Acc: 0.8931, Val Loss: 0.5431, Val Acc: 0.8413, Val F1: 0.8329\n",
            "Seed 0 | test_loss: 0.5448 | test_acc: 0.8385 | AUC: 0.8859 | Precision: 0.8274 | Recall: 0.8385 | F1: 0.8291\n",
            "\n",
            "===== SEED 2: 1 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.4572, Val Loss: 1.0985, Val Acc: 0.6327, Val F1: 0.4995\n",
            "Epoch 2/20 - Train Acc: 0.6951, Val Loss: 0.8585, Val Acc: 0.7300, Val F1: 0.6665\n",
            "Epoch 3/20 - Train Acc: 0.7442, Val Loss: 0.7657, Val Acc: 0.7496, Val F1: 0.7245\n",
            "Epoch 4/20 - Train Acc: 0.7685, Val Loss: 0.7571, Val Acc: 0.7683, Val F1: 0.7401\n",
            "Epoch 5/20 - Train Acc: 0.7862, Val Loss: 0.6677, Val Acc: 0.7975, Val F1: 0.7734\n",
            "Epoch 6/20 - Train Acc: 0.7997, Val Loss: 0.6642, Val Acc: 0.8005, Val F1: 0.7739\n",
            "Epoch 7/20 - Train Acc: 0.8122, Val Loss: 0.6055, Val Acc: 0.8010, Val F1: 0.7821\n",
            "Epoch 8/20 - Train Acc: 0.8258, Val Loss: 0.6115, Val Acc: 0.8050, Val F1: 0.7891\n",
            "Epoch 9/20 - Train Acc: 0.8336, Val Loss: 0.5839, Val Acc: 0.8196, Val F1: 0.8055\n",
            "Epoch 10/20 - Train Acc: 0.8411, Val Loss: 0.6060, Val Acc: 0.8171, Val F1: 0.8047\n",
            "Epoch 11/20 - Train Acc: 0.8491, Val Loss: 0.5636, Val Acc: 0.8111, Val F1: 0.7992\n",
            "Epoch 12/20 - Train Acc: 0.8566, Val Loss: 0.5518, Val Acc: 0.8232, Val F1: 0.8132\n",
            "Epoch 13/20 - Train Acc: 0.8629, Val Loss: 0.5619, Val Acc: 0.8207, Val F1: 0.8118\n",
            "Epoch 14/20 - Train Acc: 0.8699, Val Loss: 0.5686, Val Acc: 0.8212, Val F1: 0.8122\n",
            "Epoch 15/20 - Train Acc: 0.8748, Val Loss: 0.5824, Val Acc: 0.8232, Val F1: 0.8098\n",
            "Epoch 16/20 - Train Acc: 0.8799, Val Loss: 0.5772, Val Acc: 0.8237, Val F1: 0.8135\n",
            "Epoch 17/20 - Train Acc: 0.8856, Val Loss: 0.5728, Val Acc: 0.8232, Val F1: 0.8132\n",
            "Epoch 18/20 - Train Acc: 0.8870, Val Loss: 0.5719, Val Acc: 0.8227, Val F1: 0.8130\n",
            "Epoch 19/20 - Train Acc: 0.8892, Val Loss: 0.5701, Val Acc: 0.8212, Val F1: 0.8123\n",
            "Epoch 20/20 - Train Acc: 0.8910, Val Loss: 0.5710, Val Acc: 0.8222, Val F1: 0.8128\n",
            "Seed 1 | test_loss: 0.6007 | test_acc: 0.8213 | AUC: 0.8519 | Precision: 0.8058 | Recall: 0.8213 | F1: 0.8098\n",
            "\n",
            "===== SEED 3: 2 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5141, Val Loss: 1.0831, Val Acc: 0.6443, Val F1: 0.5164\n",
            "Epoch 2/20 - Train Acc: 0.7013, Val Loss: 0.8214, Val Acc: 0.7300, Val F1: 0.6621\n",
            "Epoch 3/20 - Train Acc: 0.7445, Val Loss: 0.7812, Val Acc: 0.7526, Val F1: 0.6999\n",
            "Epoch 4/20 - Train Acc: 0.7811, Val Loss: 0.6880, Val Acc: 0.7864, Val F1: 0.7582\n",
            "Epoch 5/20 - Train Acc: 0.8011, Val Loss: 0.6286, Val Acc: 0.8005, Val F1: 0.7743\n",
            "Epoch 6/20 - Train Acc: 0.8126, Val Loss: 0.6153, Val Acc: 0.8065, Val F1: 0.7827\n",
            "Epoch 7/20 - Train Acc: 0.8194, Val Loss: 0.5686, Val Acc: 0.8055, Val F1: 0.7990\n",
            "Epoch 8/20 - Train Acc: 0.8315, Val Loss: 0.5774, Val Acc: 0.8181, Val F1: 0.7989\n",
            "Epoch 9/20 - Train Acc: 0.8393, Val Loss: 0.5699, Val Acc: 0.8252, Val F1: 0.8096\n",
            "Epoch 10/20 - Train Acc: 0.8453, Val Loss: 0.5219, Val Acc: 0.8277, Val F1: 0.8170\n",
            "Epoch 11/20 - Train Acc: 0.8542, Val Loss: 0.5377, Val Acc: 0.8307, Val F1: 0.8197\n",
            "Epoch 12/20 - Train Acc: 0.8583, Val Loss: 0.5351, Val Acc: 0.8348, Val F1: 0.8239\n",
            "Epoch 13/20 - Train Acc: 0.8648, Val Loss: 0.5310, Val Acc: 0.8368, Val F1: 0.8258\n",
            "Epoch 14/20 - Train Acc: 0.8739, Val Loss: 0.5375, Val Acc: 0.8332, Val F1: 0.8235\n",
            "Epoch 15/20 - Train Acc: 0.8768, Val Loss: 0.5297, Val Acc: 0.8287, Val F1: 0.8212\n",
            "Epoch 16/20 - Train Acc: 0.8831, Val Loss: 0.5452, Val Acc: 0.8322, Val F1: 0.8245\n",
            "Epoch 17/20 - Train Acc: 0.8862, Val Loss: 0.5401, Val Acc: 0.8277, Val F1: 0.8182\n",
            "Epoch 18/20 - Train Acc: 0.8889, Val Loss: 0.5388, Val Acc: 0.8297, Val F1: 0.8221\n",
            "Epoch 19/20 - Train Acc: 0.8913, Val Loss: 0.5396, Val Acc: 0.8322, Val F1: 0.8244\n",
            "Epoch 20/20 - Train Acc: 0.8907, Val Loss: 0.5388, Val Acc: 0.8312, Val F1: 0.8235\n",
            "Seed 2 | test_loss: 0.5751 | test_acc: 0.8249 | AUC: 0.8517 | Precision: 0.8139 | Recall: 0.8249 | F1: 0.8161\n",
            "\n",
            "===== SEED 4: 3 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.4794, Val Loss: 1.1160, Val Acc: 0.6418, Val F1: 0.5184\n",
            "Epoch 2/20 - Train Acc: 0.6989, Val Loss: 0.8361, Val Acc: 0.7320, Val F1: 0.6669\n",
            "Epoch 3/20 - Train Acc: 0.7618, Val Loss: 0.7030, Val Acc: 0.7844, Val F1: 0.7590\n",
            "Epoch 4/20 - Train Acc: 0.7894, Val Loss: 0.6964, Val Acc: 0.7879, Val F1: 0.7567\n",
            "Epoch 5/20 - Train Acc: 0.8035, Val Loss: 0.6200, Val Acc: 0.8020, Val F1: 0.7822\n",
            "Epoch 6/20 - Train Acc: 0.8145, Val Loss: 0.6090, Val Acc: 0.8045, Val F1: 0.7869\n",
            "Epoch 7/20 - Train Acc: 0.8242, Val Loss: 0.5697, Val Acc: 0.8111, Val F1: 0.7961\n",
            "Epoch 8/20 - Train Acc: 0.8350, Val Loss: 0.5659, Val Acc: 0.8081, Val F1: 0.8005\n",
            "Epoch 9/20 - Train Acc: 0.8424, Val Loss: 0.5503, Val Acc: 0.8202, Val F1: 0.8068\n",
            "Epoch 10/20 - Train Acc: 0.8478, Val Loss: 0.5405, Val Acc: 0.8191, Val F1: 0.8089\n",
            "Epoch 11/20 - Train Acc: 0.8584, Val Loss: 0.5766, Val Acc: 0.8141, Val F1: 0.8078\n",
            "Epoch 12/20 - Train Acc: 0.8634, Val Loss: 0.5810, Val Acc: 0.8191, Val F1: 0.8059\n",
            "Epoch 13/20 - Train Acc: 0.8680, Val Loss: 0.5442, Val Acc: 0.8237, Val F1: 0.8132\n",
            "Epoch 14/20 - Train Acc: 0.8747, Val Loss: 0.5445, Val Acc: 0.8186, Val F1: 0.8104\n",
            "Epoch 15/20 - Train Acc: 0.8786, Val Loss: 0.5326, Val Acc: 0.8267, Val F1: 0.8191\n",
            "Epoch 16/20 - Train Acc: 0.8837, Val Loss: 0.5445, Val Acc: 0.8242, Val F1: 0.8153\n",
            "Epoch 17/20 - Train Acc: 0.8881, Val Loss: 0.5399, Val Acc: 0.8282, Val F1: 0.8191\n",
            "Epoch 18/20 - Train Acc: 0.8900, Val Loss: 0.5386, Val Acc: 0.8257, Val F1: 0.8180\n",
            "Epoch 19/20 - Train Acc: 0.8920, Val Loss: 0.5392, Val Acc: 0.8252, Val F1: 0.8171\n",
            "Epoch 20/20 - Train Acc: 0.8949, Val Loss: 0.5393, Val Acc: 0.8242, Val F1: 0.8160\n",
            "Seed 3 | test_loss: 0.5477 | test_acc: 0.8354 | AUC: 0.8300 | Precision: 0.8242 | Recall: 0.8354 | F1: 0.8273\n",
            "\n",
            "===== SEED 5: 4 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5088, Val Loss: 1.1073, Val Acc: 0.6458, Val F1: 0.5182\n",
            "Epoch 2/20 - Train Acc: 0.7004, Val Loss: 0.8906, Val Acc: 0.7295, Val F1: 0.6559\n",
            "Epoch 3/20 - Train Acc: 0.7480, Val Loss: 0.7674, Val Acc: 0.7647, Val F1: 0.7230\n",
            "Epoch 4/20 - Train Acc: 0.7854, Val Loss: 0.7142, Val Acc: 0.7824, Val F1: 0.7669\n",
            "Epoch 5/20 - Train Acc: 0.8049, Val Loss: 0.6237, Val Acc: 0.7980, Val F1: 0.7811\n",
            "Epoch 6/20 - Train Acc: 0.8182, Val Loss: 0.6090, Val Acc: 0.8131, Val F1: 0.7966\n",
            "Epoch 7/20 - Train Acc: 0.8281, Val Loss: 0.7196, Val Acc: 0.7940, Val F1: 0.7652\n",
            "Epoch 8/20 - Train Acc: 0.8374, Val Loss: 0.5586, Val Acc: 0.8186, Val F1: 0.8069\n",
            "Epoch 9/20 - Train Acc: 0.8453, Val Loss: 0.5743, Val Acc: 0.8126, Val F1: 0.8011\n",
            "Epoch 10/20 - Train Acc: 0.8480, Val Loss: 0.5816, Val Acc: 0.8252, Val F1: 0.8096\n",
            "Epoch 11/20 - Train Acc: 0.8556, Val Loss: 0.5747, Val Acc: 0.8227, Val F1: 0.8093\n",
            "Epoch 12/20 - Train Acc: 0.8649, Val Loss: 0.5727, Val Acc: 0.8297, Val F1: 0.8168\n",
            "Epoch 13/20 - Train Acc: 0.8702, Val Loss: 0.5617, Val Acc: 0.8227, Val F1: 0.8117\n",
            "Epoch 14/20 - Train Acc: 0.8744, Val Loss: 0.5595, Val Acc: 0.8227, Val F1: 0.8134\n",
            "Epoch 15/20 - Train Acc: 0.8784, Val Loss: 0.5636, Val Acc: 0.8262, Val F1: 0.8162\n",
            "Epoch 16/20 - Train Acc: 0.8840, Val Loss: 0.5853, Val Acc: 0.8242, Val F1: 0.8127\n",
            "Epoch 17/20 - Train Acc: 0.8883, Val Loss: 0.5735, Val Acc: 0.8252, Val F1: 0.8152\n",
            "Epoch 18/20 - Train Acc: 0.8893, Val Loss: 0.5751, Val Acc: 0.8252, Val F1: 0.8151\n",
            "Epoch 19/20 - Train Acc: 0.8915, Val Loss: 0.5759, Val Acc: 0.8222, Val F1: 0.8126\n",
            "Epoch 20/20 - Train Acc: 0.8945, Val Loss: 0.5763, Val Acc: 0.8232, Val F1: 0.8134\n",
            "Seed 4 | test_loss: 0.5889 | test_acc: 0.8181 | AUC: 0.8471 | Precision: 0.8020 | Recall: 0.8181 | F1: 0.8069\n",
            "\n",
            "===== SEED 6: 5 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5129, Val Loss: 1.1270, Val Acc: 0.6469, Val F1: 0.5240\n",
            "Epoch 2/20 - Train Acc: 0.6895, Val Loss: 0.8379, Val Acc: 0.7340, Val F1: 0.6625\n",
            "Epoch 3/20 - Train Acc: 0.7385, Val Loss: 0.7619, Val Acc: 0.7547, Val F1: 0.7059\n",
            "Epoch 4/20 - Train Acc: 0.7762, Val Loss: 0.6464, Val Acc: 0.7919, Val F1: 0.7727\n",
            "Epoch 5/20 - Train Acc: 0.7989, Val Loss: 0.6172, Val Acc: 0.8060, Val F1: 0.7826\n",
            "Epoch 6/20 - Train Acc: 0.8108, Val Loss: 0.6396, Val Acc: 0.7995, Val F1: 0.7745\n",
            "Epoch 7/20 - Train Acc: 0.8213, Val Loss: 0.5472, Val Acc: 0.8237, Val F1: 0.8065\n",
            "Epoch 8/20 - Train Acc: 0.8296, Val Loss: 0.5245, Val Acc: 0.8292, Val F1: 0.8179\n",
            "Epoch 9/20 - Train Acc: 0.8394, Val Loss: 0.5352, Val Acc: 0.8287, Val F1: 0.8168\n",
            "Epoch 10/20 - Train Acc: 0.8464, Val Loss: 0.5491, Val Acc: 0.8202, Val F1: 0.8152\n",
            "Epoch 11/20 - Train Acc: 0.8532, Val Loss: 0.5475, Val Acc: 0.8176, Val F1: 0.8062\n",
            "Epoch 12/20 - Train Acc: 0.8576, Val Loss: 0.5433, Val Acc: 0.8312, Val F1: 0.8174\n",
            "Epoch 13/20 - Train Acc: 0.8679, Val Loss: 0.5321, Val Acc: 0.8262, Val F1: 0.8157\n",
            "Epoch 14/20 - Train Acc: 0.8731, Val Loss: 0.5402, Val Acc: 0.8257, Val F1: 0.8186\n",
            "Epoch 15/20 - Train Acc: 0.8766, Val Loss: 0.5436, Val Acc: 0.8267, Val F1: 0.8193\n",
            "Epoch 16/20 - Train Acc: 0.8836, Val Loss: 0.5478, Val Acc: 0.8292, Val F1: 0.8190\n",
            "Epoch 17/20 - Train Acc: 0.8877, Val Loss: 0.5604, Val Acc: 0.8277, Val F1: 0.8181\n",
            "Epoch 18/20 - Train Acc: 0.8900, Val Loss: 0.5516, Val Acc: 0.8257, Val F1: 0.8176\n",
            "Epoch 19/20 - Train Acc: 0.8916, Val Loss: 0.5532, Val Acc: 0.8272, Val F1: 0.8183\n",
            "Epoch 20/20 - Train Acc: 0.8940, Val Loss: 0.5531, Val Acc: 0.8292, Val F1: 0.8204\n",
            "Seed 5 | test_loss: 0.5586 | test_acc: 0.8227 | AUC: 0.9078 | Precision: 0.8091 | Recall: 0.8227 | F1: 0.8128\n",
            "\n",
            "===== SEED 7: 6 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5250, Val Loss: 1.0835, Val Acc: 0.6398, Val F1: 0.5106\n",
            "Epoch 2/20 - Train Acc: 0.6920, Val Loss: 0.8644, Val Acc: 0.7229, Val F1: 0.6520\n",
            "Epoch 3/20 - Train Acc: 0.7319, Val Loss: 0.7801, Val Acc: 0.7511, Val F1: 0.7085\n",
            "Epoch 4/20 - Train Acc: 0.7667, Val Loss: 0.7010, Val Acc: 0.7783, Val F1: 0.7588\n",
            "Epoch 5/20 - Train Acc: 0.7923, Val Loss: 0.6572, Val Acc: 0.7940, Val F1: 0.7772\n",
            "Epoch 6/20 - Train Acc: 0.8123, Val Loss: 0.6608, Val Acc: 0.8020, Val F1: 0.7776\n",
            "Epoch 7/20 - Train Acc: 0.8219, Val Loss: 0.6118, Val Acc: 0.7950, Val F1: 0.7838\n",
            "Epoch 8/20 - Train Acc: 0.8297, Val Loss: 0.5941, Val Acc: 0.8050, Val F1: 0.7914\n",
            "Epoch 9/20 - Train Acc: 0.8370, Val Loss: 0.5819, Val Acc: 0.8141, Val F1: 0.7985\n",
            "Epoch 10/20 - Train Acc: 0.8459, Val Loss: 0.5938, Val Acc: 0.8086, Val F1: 0.7948\n",
            "Epoch 11/20 - Train Acc: 0.8508, Val Loss: 0.5831, Val Acc: 0.8116, Val F1: 0.8002\n",
            "Epoch 12/20 - Train Acc: 0.8586, Val Loss: 0.5831, Val Acc: 0.8191, Val F1: 0.8035\n",
            "Epoch 13/20 - Train Acc: 0.8632, Val Loss: 0.6069, Val Acc: 0.8156, Val F1: 0.7987\n",
            "Epoch 14/20 - Train Acc: 0.8686, Val Loss: 0.5845, Val Acc: 0.8232, Val F1: 0.8076\n",
            "Epoch 15/20 - Train Acc: 0.8720, Val Loss: 0.5742, Val Acc: 0.8237, Val F1: 0.8108\n",
            "Epoch 16/20 - Train Acc: 0.8771, Val Loss: 0.5726, Val Acc: 0.8207, Val F1: 0.8090\n",
            "Epoch 17/20 - Train Acc: 0.8810, Val Loss: 0.5727, Val Acc: 0.8252, Val F1: 0.8131\n",
            "Epoch 18/20 - Train Acc: 0.8835, Val Loss: 0.5751, Val Acc: 0.8227, Val F1: 0.8107\n",
            "Epoch 19/20 - Train Acc: 0.8853, Val Loss: 0.5729, Val Acc: 0.8247, Val F1: 0.8132\n",
            "Epoch 20/20 - Train Acc: 0.8849, Val Loss: 0.5722, Val Acc: 0.8237, Val F1: 0.8129\n",
            "Seed 6 | test_loss: 0.5305 | test_acc: 0.8236 | AUC: 0.9264 | Precision: 0.8105 | Recall: 0.8236 | F1: 0.8150\n",
            "\n",
            "===== SEED 8: 7 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5108, Val Loss: 1.1397, Val Acc: 0.6287, Val F1: 0.4953\n",
            "Epoch 2/20 - Train Acc: 0.6868, Val Loss: 0.8880, Val Acc: 0.7254, Val F1: 0.6827\n",
            "Epoch 3/20 - Train Acc: 0.7469, Val Loss: 0.7231, Val Acc: 0.7914, Val F1: 0.7635\n",
            "Epoch 4/20 - Train Acc: 0.7856, Val Loss: 0.6566, Val Acc: 0.8000, Val F1: 0.7765\n",
            "Epoch 5/20 - Train Acc: 0.7999, Val Loss: 0.6373, Val Acc: 0.7970, Val F1: 0.7822\n",
            "Epoch 6/20 - Train Acc: 0.8105, Val Loss: 0.6095, Val Acc: 0.8091, Val F1: 0.7911\n",
            "Epoch 7/20 - Train Acc: 0.8226, Val Loss: 0.5951, Val Acc: 0.8176, Val F1: 0.7980\n",
            "Epoch 8/20 - Train Acc: 0.8332, Val Loss: 0.5673, Val Acc: 0.8156, Val F1: 0.8065\n",
            "Epoch 9/20 - Train Acc: 0.8373, Val Loss: 0.5615, Val Acc: 0.8267, Val F1: 0.8100\n",
            "Epoch 10/20 - Train Acc: 0.8475, Val Loss: 0.5831, Val Acc: 0.8222, Val F1: 0.8024\n",
            "Epoch 11/20 - Train Acc: 0.8577, Val Loss: 0.5334, Val Acc: 0.8267, Val F1: 0.8188\n",
            "Epoch 12/20 - Train Acc: 0.8632, Val Loss: 0.5563, Val Acc: 0.8247, Val F1: 0.8173\n",
            "Epoch 13/20 - Train Acc: 0.8716, Val Loss: 0.5416, Val Acc: 0.8302, Val F1: 0.8167\n",
            "Epoch 14/20 - Train Acc: 0.8764, Val Loss: 0.5512, Val Acc: 0.8252, Val F1: 0.8140\n",
            "Epoch 15/20 - Train Acc: 0.8845, Val Loss: 0.5554, Val Acc: 0.8232, Val F1: 0.8137\n",
            "Epoch 16/20 - Train Acc: 0.8884, Val Loss: 0.5531, Val Acc: 0.8257, Val F1: 0.8193\n",
            "Epoch 17/20 - Train Acc: 0.8925, Val Loss: 0.5545, Val Acc: 0.8237, Val F1: 0.8154\n",
            "Epoch 18/20 - Train Acc: 0.8966, Val Loss: 0.5506, Val Acc: 0.8297, Val F1: 0.8208\n",
            "Epoch 19/20 - Train Acc: 0.8995, Val Loss: 0.5542, Val Acc: 0.8307, Val F1: 0.8207\n",
            "Epoch 20/20 - Train Acc: 0.8984, Val Loss: 0.5528, Val Acc: 0.8312, Val F1: 0.8210\n",
            "Seed 7 | test_loss: 0.6173 | test_acc: 0.8200 | AUC: 0.9255 | Precision: 0.8039 | Recall: 0.8200 | F1: 0.8096\n",
            "\n",
            "===== SEED 9: 8 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.4786, Val Loss: 1.1118, Val Acc: 0.6584, Val F1: 0.5566\n",
            "Epoch 2/20 - Train Acc: 0.6836, Val Loss: 0.8876, Val Acc: 0.7224, Val F1: 0.6461\n",
            "Epoch 3/20 - Train Acc: 0.7334, Val Loss: 0.7798, Val Acc: 0.7562, Val F1: 0.7108\n",
            "Epoch 4/20 - Train Acc: 0.7695, Val Loss: 0.7028, Val Acc: 0.7778, Val F1: 0.7553\n",
            "Epoch 5/20 - Train Acc: 0.7960, Val Loss: 0.6406, Val Acc: 0.8015, Val F1: 0.7803\n",
            "Epoch 6/20 - Train Acc: 0.8095, Val Loss: 0.6139, Val Acc: 0.8126, Val F1: 0.7910\n",
            "Epoch 7/20 - Train Acc: 0.8198, Val Loss: 0.6072, Val Acc: 0.8116, Val F1: 0.7949\n",
            "Epoch 8/20 - Train Acc: 0.8281, Val Loss: 0.5822, Val Acc: 0.8207, Val F1: 0.8005\n",
            "Epoch 9/20 - Train Acc: 0.8382, Val Loss: 0.6047, Val Acc: 0.8151, Val F1: 0.7906\n",
            "Epoch 10/20 - Train Acc: 0.8433, Val Loss: 0.5433, Val Acc: 0.8232, Val F1: 0.8102\n",
            "Epoch 11/20 - Train Acc: 0.8510, Val Loss: 0.5345, Val Acc: 0.8277, Val F1: 0.8147\n",
            "Epoch 12/20 - Train Acc: 0.8585, Val Loss: 0.5678, Val Acc: 0.8287, Val F1: 0.8113\n",
            "Epoch 13/20 - Train Acc: 0.8635, Val Loss: 0.5634, Val Acc: 0.8146, Val F1: 0.8058\n",
            "Epoch 14/20 - Train Acc: 0.8698, Val Loss: 0.5510, Val Acc: 0.8302, Val F1: 0.8163\n",
            "Epoch 15/20 - Train Acc: 0.8777, Val Loss: 0.5634, Val Acc: 0.8272, Val F1: 0.8111\n",
            "Epoch 16/20 - Train Acc: 0.8807, Val Loss: 0.5508, Val Acc: 0.8287, Val F1: 0.8163\n",
            "Epoch 17/20 - Train Acc: 0.8845, Val Loss: 0.5542, Val Acc: 0.8322, Val F1: 0.8179\n",
            "Epoch 18/20 - Train Acc: 0.8868, Val Loss: 0.5474, Val Acc: 0.8302, Val F1: 0.8180\n",
            "Epoch 19/20 - Train Acc: 0.8887, Val Loss: 0.5509, Val Acc: 0.8302, Val F1: 0.8183\n",
            "Epoch 20/20 - Train Acc: 0.8897, Val Loss: 0.5535, Val Acc: 0.8302, Val F1: 0.8177\n",
            "Seed 8 | test_loss: 0.5259 | test_acc: 0.8317 | AUC: 0.8968 | Precision: 0.8128 | Recall: 0.8317 | F1: 0.8186\n",
            "\n",
            "===== SEED 10: 9 =====\n",
            "N=22046, num_classes=11, device=cuda\n",
            "Epoch 1/20 - Train Acc: 0.5331, Val Loss: 1.1253, Val Acc: 0.6307, Val F1: 0.4879\n",
            "Epoch 2/20 - Train Acc: 0.6989, Val Loss: 0.8450, Val Acc: 0.7310, Val F1: 0.6619\n",
            "Epoch 3/20 - Train Acc: 0.7466, Val Loss: 0.7578, Val Acc: 0.7476, Val F1: 0.7155\n",
            "Epoch 4/20 - Train Acc: 0.7725, Val Loss: 0.7036, Val Acc: 0.7768, Val F1: 0.7404\n",
            "Epoch 5/20 - Train Acc: 0.7903, Val Loss: 0.6520, Val Acc: 0.7950, Val F1: 0.7682\n",
            "Epoch 6/20 - Train Acc: 0.8031, Val Loss: 0.5972, Val Acc: 0.8000, Val F1: 0.7854\n",
            "Epoch 7/20 - Train Acc: 0.8189, Val Loss: 0.5814, Val Acc: 0.8161, Val F1: 0.8000\n",
            "Epoch 8/20 - Train Acc: 0.8273, Val Loss: 0.5577, Val Acc: 0.8212, Val F1: 0.8050\n",
            "Epoch 9/20 - Train Acc: 0.8380, Val Loss: 0.5389, Val Acc: 0.8267, Val F1: 0.8117\n",
            "Epoch 10/20 - Train Acc: 0.8431, Val Loss: 0.5414, Val Acc: 0.8267, Val F1: 0.8126\n",
            "Epoch 11/20 - Train Acc: 0.8519, Val Loss: 0.5623, Val Acc: 0.8247, Val F1: 0.8075\n",
            "Epoch 12/20 - Train Acc: 0.8586, Val Loss: 0.5721, Val Acc: 0.8262, Val F1: 0.8098\n",
            "Epoch 13/20 - Train Acc: 0.8681, Val Loss: 0.5532, Val Acc: 0.8202, Val F1: 0.8082\n",
            "Epoch 14/20 - Train Acc: 0.8707, Val Loss: 0.5514, Val Acc: 0.8302, Val F1: 0.8156\n",
            "Epoch 15/20 - Train Acc: 0.8774, Val Loss: 0.5700, Val Acc: 0.8222, Val F1: 0.8096\n",
            "Epoch 16/20 - Train Acc: 0.8827, Val Loss: 0.5571, Val Acc: 0.8262, Val F1: 0.8148\n",
            "Epoch 17/20 - Train Acc: 0.8878, Val Loss: 0.5694, Val Acc: 0.8312, Val F1: 0.8184\n",
            "Epoch 18/20 - Train Acc: 0.8889, Val Loss: 0.5595, Val Acc: 0.8267, Val F1: 0.8152\n",
            "Epoch 19/20 - Train Acc: 0.8909, Val Loss: 0.5622, Val Acc: 0.8237, Val F1: 0.8110\n",
            "Epoch 20/20 - Train Acc: 0.8942, Val Loss: 0.5626, Val Acc: 0.8257, Val F1: 0.8129\n",
            "Seed 9 | test_loss: 0.6182 | test_acc: 0.8091 | AUC: 0.8962 | Precision: 0.7951 | Recall: 0.8091 | F1: 0.7980\n",
            "\n",
            "===== SUMMARY ACROSS 10 SEEDS =====\n",
            "Average Test Accuracy: 0.8245 ± 0.0083\n",
            "Average Test AUC: 0.8819 ± 0.0327\n",
            "Average Test Precision: 0.8105 ± 0.0093\n",
            "Average Test Recall: 0.8245 ± 0.0083\n",
            "Average Test F1: 0.8143 ± 0.0088\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 1e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = int(torch.max(y).item() + 1)\n",
        "all_results = []\n",
        "\n",
        "seeds = list(range(10))\n",
        "\n",
        "for seed_idx, seed in enumerate(seeds, 1):\n",
        "    print(f\"\\n===== SEED {seed_idx}: {seed} =====\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    assert N == y.shape[0], \"X and y must have same first dimension\"\n",
        "    num_classes = int(torch.max(y).item() + 1)\n",
        "    print(f\"N={N}, num_classes={num_classes}, device={device}\")\n",
        "\n",
        "    all_indices = list(range(len(X)))\n",
        "    all_labels = y.numpy()\n",
        "\n",
        "    trainval_indices, test_indices = train_test_split(\n",
        "        all_indices,\n",
        "        test_size=0.1,\n",
        "        random_state=seed,\n",
        "        stratify=all_labels\n",
        "    )\n",
        "\n",
        "    trainval_labels = [all_labels[i] for i in trainval_indices]\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        trainval_indices,\n",
        "        test_size=0.1,\n",
        "        random_state=seed,\n",
        "        stratify=trainval_labels\n",
        "    )\n",
        "\n",
        "    train_ds = TensorDataset(X[train_indices], y[train_indices])\n",
        "    val_ds = TensorDataset(X[val_indices], y[val_indices])\n",
        "    test_ds = TensorDataset(X[test_indices], y[test_indices])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ECGModel(sequence_length=X.shape[1], num_channels=X.shape[2],\n",
        "                     d_model=128, num_heads=4, dff=128, dropout_rate=0.3,\n",
        "                     num_classes=num_classes, apply_softmax=False).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xb_dummy = X[train_indices[:2]].to(device)\n",
        "        _ = model(xb_dummy)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    param_names = [n for n, _ in model.named_parameters()]\n",
        "    assert any(\"fc1\" in n for n in param_names)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=epochs,\n",
        "        pct_start=0.1 \n",
        "    )\n",
        "\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_correct = 0\n",
        "        seen = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clipping\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            seen += xb.size(0)\n",
        "\n",
        "        train_acc = running_correct / seen\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc, val_precision, val_recall, val_auc, val_f1 = evaluate_on_loader(\n",
        "            model, val_loader, criterion, device, num_classes\n",
        "        )\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    test_loss, test_acc, test_precision, test_recall, test_auc, test_f1 = evaluate_on_loader(\n",
        "        model, test_loader, criterion, device, num_classes\n",
        "    )\n",
        "\n",
        "    print(f\"Seed {seed} | test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | \"\n",
        "          f\"AUC: {test_auc:.4f} | Precision: {test_precision:.4f} | \"\n",
        "          f\"Recall: {test_recall:.4f} | F1: {test_f1:.4f}\")\n",
        "\n",
        "    all_results.append({\n",
        "        'seed': seed,\n",
        "        'train_acc_list': train_acc_list,\n",
        "        'val_acc_list': val_acc_list,\n",
        "        'test_acc': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'test_auc': test_auc,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'test_f1': test_f1\n",
        "    })\n",
        "\n",
        "# Summary across seeds\n",
        "test_accs = [r['test_acc'] for r in all_results]\n",
        "test_aucs = [r['test_auc'] for r in all_results if not np.isnan(r['test_auc'])]\n",
        "test_precisions = [r['test_precision'] for r in all_results]\n",
        "test_recalls = [r['test_recall'] for r in all_results]\n",
        "test_f1s = [r['test_f1'] for r in all_results]\n",
        "\n",
        "print(f\"\\n===== SUMMARY ACROSS {len(seeds)} SEEDS =====\")\n",
        "print(f\"Average Test Accuracy: {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
        "if test_aucs:\n",
        "    print(f\"Average Test AUC: {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n",
        "else:\n",
        "    print(f\"Average Test AUC: Could not be calculated\")\n",
        "print(f\"Average Test Precision: {np.mean(test_precisions):.4f} ± {np.std(test_precisions):.4f}\")\n",
        "print(f\"Average Test Recall: {np.mean(test_recalls):.4f} ± {np.std(test_recalls):.4f}\")\n",
        "print(f\"Average Test F1: {np.mean(test_f1s):.4f} ± {np.std(test_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHFCfjtgZQjQ"
      },
      "source": [
        "# -----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_WcZ0O7sPda"
      },
      "outputs": [],
      "source": [
        "!pip install thop calflops -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo364g1fx_gv",
        "outputId": "d1497fa5-74d9-4dda-e3fc-2d452251aa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------- Calculate Flops Results -------------------------------------\n",
            "Notations:\n",
            "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
            "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
            "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
            "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
            "\n",
            "Total Training Params:                                                  10.65 M \n",
            "fwd MACs:                                                               362.893 MMACs\n",
            "fwd FLOPs:                                                              732.148 MFLOPS\n",
            "fwd+bwd MACs:                                                           1.0887 GMACs\n",
            "fwd+bwd FLOPs:                                                          2.1964 GFLOPS\n",
            "\n",
            "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
            "Each module caculated is listed after its name in the following order: \n",
            "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
            "\n",
            "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
            " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
            "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
            "\n",
            "ECGModel(\n",
            "  10.65 M = 100% Params, 362.89 MMACs = 100% MACs, 732.15 MFLOPS = 100% FLOPs\n",
            "  (conv1): Conv1d(8.1 K = 0.076% Params, 40.32 MMACs = 11.1107% MACs, 80.8 MFLOPS = 11.036% FLOPs, 12, 32, kernel_size=(21,), stride=(1,), padding=(10,))\n",
            "  (bn1): GroupNorm(64 = 0.0006% Params, 0 MACs = 0% MACs, 800 KFLOPS = 0.1093% FLOPs, 8, 32, eps=1e-05, affine=True)\n",
            "  (ca1): ChannelAttention(\n",
            "    292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 321.03 KFLOPS = 0.0438% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 1.03 KFLOPS = 0.0001% FLOPs\n",
            "      (0): Linear(132 = 0.0012% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=32, out_features=4, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 8 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(160 = 0.0015% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=4, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool1): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 160 KFLOPS = 0.0219% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv1d(23.58 K = 0.2214% Params, 58.88 MMACs = 16.2252% MACs, 117.84 MFLOPS = 16.0951% FLOPs, 32, 32, kernel_size=(23,), stride=(1,), padding=(11,))\n",
            "  (bn2): GroupNorm(64 = 0.0006% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0546% FLOPs, 8, 32, eps=1e-05, affine=True)\n",
            "  (ca2): ChannelAttention(\n",
            "    292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 161.03 KFLOPS = 0.022% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      292 = 0.0027% Params, 512 MACs = 0.0001% MACs, 1.03 KFLOPS = 0.0001% FLOPs\n",
            "      (0): Linear(132 = 0.0012% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=32, out_features=4, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 8 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(160 = 0.0015% Params, 256 MACs = 0.0001% MACs, 512 FLOPS = 0.0001% FLOPs, in_features=4, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool2): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0109% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv1d(51.26 K = 0.4813% Params, 64 MMACs = 17.6361% MACs, 128.08 MFLOPS = 17.4937% FLOPs, 32, 64, kernel_size=(25,), stride=(1,), padding=(12,))\n",
            "  (bn3): GroupNorm(128 = 0.0012% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0546% FLOPs, 8, 64, eps=1e-05, affine=True)\n",
            "  (ca3): ChannelAttention(\n",
            "    1.1 K = 0.0103% Params, 2.05 KMACs = 0.0006% MACs, 164.11 KFLOPS = 0.0224% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      1.1 K = 0.0103% Params, 2.05 KMACs = 0.0006% MACs, 4.11 KFLOPS = 0.0006% FLOPs\n",
            "      (0): Linear(520 = 0.0049% Params, 1.02 KMACs = 0.0003% MACs, 2.05 KFLOPS = 0.0003% FLOPs, in_features=64, out_features=8, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 16 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(576 = 0.0054% Params, 1.02 KMACs = 0.0003% MACs, 2.05 KFLOPS = 0.0003% FLOPs, in_features=8, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (pool3): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0109% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv1d(221.31 K = 2.0777% Params, 138.24 MMACs = 38.0939% MACs, 276.56 MFLOPS = 37.7738% FLOPs, 64, 128, kernel_size=(27,), stride=(1,), padding=(13,))\n",
            "  (bn4): GroupNorm(256 = 0.0024% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0546% FLOPs, 8, 128, eps=1e-05, affine=True)\n",
            "  (ca4): ChannelAttention(\n",
            "    4.24 K = 0.0398% Params, 8.19 KMACs = 0.0023% MACs, 176.42 KFLOPS = 0.0241% FLOPs\n",
            "    (mlp): Sequential(\n",
            "      4.24 K = 0.0398% Params, 8.19 KMACs = 0.0023% MACs, 16.42 KFLOPS = 0.0022% FLOPs\n",
            "      (0): Linear(2.06 K = 0.0194% Params, 4.1 KMACs = 0.0011% MACs, 8.19 KFLOPS = 0.0011% FLOPs, in_features=128, out_features=16, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0% FLOPs, inplace=True)\n",
            "      (2): Linear(2.18 K = 0.0204% Params, 4.1 KMACs = 0.0011% MACs, 8.19 KFLOPS = 0.0011% FLOPs, in_features=16, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (transformer_block): TransformerEncoderBlock(\n",
            "    99.58 K = 0.9349% Params, 61.44 MMACs = 16.9306% MACs, 125.32 MFLOPS = 17.1171% FLOPs\n",
            "    (mha): MultiheadAttention(\n",
            "      66.05 K = 0.6201% Params, 40.96 MMACs = 11.2871% MACs, 83.48 MFLOPS = 11.4024% FLOPs\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(16.51 K = 0.155% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (dropout1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)\n",
            "    (norm1): LayerNorm(256 = 0.0024% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0546% FLOPs, (128,), eps=1e-06, elementwise_affine=True)\n",
            "    (ffn): Sequential(\n",
            "      33.02 K = 0.31% Params, 20.48 MMACs = 5.6435% MACs, 41.04 MFLOPS = 5.6054% FLOPs\n",
            "      (0): Linear(16.51 K = 0.155% Params, 10.24 MMACs = 2.8218% MACs, 20.48 MFLOPS = 2.7972% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "      (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 80 KFLOPS = 0.0109% FLOPs, inplace=True)\n",
            "      (2): Linear(16.51 K = 0.155% Params, 10.24 MMACs = 2.8218% MACs, 20.48 MFLOPS = 2.7972% FLOPs, in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (dropout2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)\n",
            "    (norm2): LayerNorm(256 = 0.0024% Params, 0 MACs = 0% MACs, 400 KFLOPS = 0.0546% FLOPs, (128,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (flatten): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
            "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)\n",
            "  (fc_out): Linear(1.42 K = 0.0133% Params, 1.41 KMACs = 0.0004% MACs, 2.82 KFLOPS = 0.0004% FLOPs, in_features=128, out_features=11, bias=True)\n",
            "  (fc1): Linear(10.24 M = 96.135% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=80000, out_features=128, bias=True)\n",
            ")\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model FLOPs:732.148 MFLOPS   MACs:362.893 MMACs   Params:10.6518 M \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import calflops\n",
        "\n",
        "flops, macs, params = calflops.calculate_flops(model=model,\n",
        "                                      input_shape=(1, 5000, 12),\n",
        "                                      output_as_string=True,\n",
        "                                      output_precision=4)\n",
        "print(\"Model FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvSJtvqSsAbA",
        "outputId": "73f28deb-738e-4e9c-df14-4cc489431842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL COMPLEXITY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "📊 Parameter Count:\n",
            "   Total Parameters:       10,651,819\n",
            "   Trainable Parameters:   10,651,819\n",
            "   Non-trainable Parameters: 0\n",
            "   Model Size (MB):        40.63\n",
            "\n",
            "🔢 FLOPs (Floating Point Operations):\n",
            "   MACs (Multiply-Accumulate): 322.573M\n",
            "   Parameters (thop):          345.131K\n",
            "\n",
            "📋 Layer-wise Parameter Breakdown:\n",
            "Layer Name                                    Parameters   % of Total\n",
            "----------------------------------------------------------------------\n",
            "conv1.weight                                       8,064        0.08%\n",
            "conv1.bias                                            32        0.00%\n",
            "bn1.weight                                            32        0.00%\n",
            "bn1.bias                                              32        0.00%\n",
            "ca1.mlp.0.weight                                     128        0.00%\n",
            "ca1.mlp.0.bias                                         4        0.00%\n",
            "ca1.mlp.2.weight                                     128        0.00%\n",
            "ca1.mlp.2.bias                                        32        0.00%\n",
            "conv2.weight                                      23,552        0.22%\n",
            "conv2.bias                                            32        0.00%\n",
            "bn2.weight                                            32        0.00%\n",
            "bn2.bias                                              32        0.00%\n",
            "ca2.mlp.0.weight                                     128        0.00%\n",
            "ca2.mlp.0.bias                                         4        0.00%\n",
            "ca2.mlp.2.weight                                     128        0.00%\n",
            "ca2.mlp.2.bias                                        32        0.00%\n",
            "conv3.weight                                      51,200        0.48%\n",
            "conv3.bias                                            64        0.00%\n",
            "bn3.weight                                            64        0.00%\n",
            "bn3.bias                                              64        0.00%\n",
            "ca3.mlp.0.weight                                     512        0.00%\n",
            "ca3.mlp.0.bias                                         8        0.00%\n",
            "ca3.mlp.2.weight                                     512        0.00%\n",
            "ca3.mlp.2.bias                                        64        0.00%\n",
            "conv4.weight                                     221,184        2.08%\n",
            "conv4.bias                                           128        0.00%\n",
            "bn4.weight                                           128        0.00%\n",
            "bn4.bias                                             128        0.00%\n",
            "ca4.mlp.0.weight                                   2,048        0.02%\n",
            "ca4.mlp.0.bias                                        16        0.00%\n",
            "ca4.mlp.2.weight                                   2,048        0.02%\n",
            "ca4.mlp.2.bias                                       128        0.00%\n",
            "transformer_block.mha.in_proj_weight              49,152        0.46%\n",
            "transformer_block.mha.in_proj_bias                   384        0.00%\n",
            "transformer_block.mha.out_proj.weight             16,384        0.15%\n",
            "transformer_block.mha.out_proj.bias                  128        0.00%\n",
            "transformer_block.norm1.weight                       128        0.00%\n",
            "transformer_block.norm1.bias                         128        0.00%\n",
            "transformer_block.ffn.0.weight                    16,384        0.15%\n",
            "transformer_block.ffn.0.bias                         128        0.00%\n",
            "transformer_block.ffn.2.weight                    16,384        0.15%\n",
            "transformer_block.ffn.2.bias                         128        0.00%\n",
            "transformer_block.norm2.weight                       128        0.00%\n",
            "transformer_block.norm2.bias                         128        0.00%\n",
            "fc_out.weight                                      1,408        0.01%\n",
            "fc_out.bias                                           11        0.00%\n",
            "fc1.weight                                    10,240,000       96.13%\n",
            "fc1.bias                                             128        0.00%\n",
            "\n",
            "📝 Detailed Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'total_params': 10651819,\n",
              " 'trainable_params': 10651819,\n",
              " 'model_size_mb': 40.63346481323242}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "from thop import profile, clever_format\n",
        "\n",
        "def analyze_model_complexity(model, input_size=(1, 5000, 12), device='cuda'):\n",
        "    \"\"\"\n",
        "    Analyze model complexity: parameters, FLOPs, memory\n",
        "\n",
        "    Args:\n",
        "        model: Your HANWithAttention model\n",
        "        input_size: (batch, segments, timesteps, channels)\n",
        "        device: 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODEL COMPLEXITY ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable_params = total_params - trainable_params\n",
        "\n",
        "    print(f\"\\nParameter Count:\")\n",
        "    print(f\"   Total Parameters:       {total_params:,}\")\n",
        "    print(f\"   Trainable Parameters:   {trainable_params:,}\")\n",
        "    print(f\"   Non-trainable Parameters: {non_trainable_params:,}\")\n",
        "    print(f\"   Model Size (MB):        {total_params * 4 / (1024**2):.2f}\")  # 4 bytes per float32\n",
        "\n",
        "    # 2. FLOPs calculation\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "\n",
        "    try:\n",
        "        macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "        macs, params = clever_format([macs, params], \"%.3f\")\n",
        "        print(f\"\\nFLOPs (Floating Point Operations):\")\n",
        "        print(f\"   MACs (Multiply-Accumulate): {macs}\")\n",
        "        print(f\"   Parameters (thop):          {params}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFLOPs calculation failed: {e}\")\n",
        "\n",
        "    # 3. Layer-wise parameter breakdown\n",
        "    print(f\"\\nLayer-wise Parameter Breakdown:\")\n",
        "    print(f\"{'Layer Name':<40} {'Parameters':>15} {'% of Total':>12}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            param_count = param.numel()\n",
        "            percentage = 100 * param_count / trainable_params\n",
        "            print(f\"{name:<40} {param_count:>15,} {percentage:>11.2f}%\")\n",
        "\n",
        "    # 4. Detailed model summary\n",
        "    print(f\"\\nDetailed Model Architecture:\")\n",
        "    summary(model,\n",
        "            input_size=input_size,\n",
        "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
        "            depth=4,\n",
        "            device=device,\n",
        "            verbose=0)\n",
        "\n",
        "    return {\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'model_size_mb': total_params * 4 / (1024**2)\n",
        "    }\n",
        "\n",
        "analyze_model_complexity(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9jBP_14kCSB",
        "outputId": "2eeac13b-fb14-406c-9e9e-030f794f7ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model size: 40.634MB\n"
          ]
        }
      ],
      "source": [
        "# From: https://discuss.pytorch.org/t/finding-model-size/130275\n",
        "\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMRa5bX6sZlS",
        "outputId": "4871af55-00a4-4a43-ac22-6907cf16b614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INFERENCE BENCHMARK\n",
            "================================================================================\n",
            "Device: cpu\n",
            "Input shape: (1, 5000, 12)\n",
            "Throughput batch size: 16\n",
            "\n",
            "Batch-1 Latency (ms):\n",
            "     mean_ms: 10.590\n",
            "      std_ms: 0.199\n",
            "   median_ms: 10.597\n",
            "      min_ms: 10.227\n",
            "      max_ms: 11.631\n",
            "\n",
            "Batch-N Throughput:\n",
            "     batch_time_ms: 63.960\n",
            "     per_sample_ms: 3.997\n",
            "   samples_per_sec: 250.16\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark Environment Setup\n",
        "# =============================================================================\n",
        "\n",
        "def setup_benchmark_env():\n",
        "    \"\"\"\n",
        "    Freeze backend behavior for reproducible benchmarking.\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = False\n",
        "    torch.backends.cudnn.allow_tf32 = False\n",
        "\n",
        "def time_forward(model, inputs, device='cuda'):\n",
        "    \"\"\"\n",
        "    Time a single forward pass with proper synchronization.\n",
        "    Returns elapsed time in seconds.\n",
        "    \"\"\"\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "        _ = model(inputs)\n",
        "        end.record()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        return start.elapsed_time(end) / 1000.0  # seconds\n",
        "    else:\n",
        "        t0 = time.time()\n",
        "        _ = model(inputs)\n",
        "        return time.time() - t0\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Batch-1 Latency Benchmark\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark_latency(\n",
        "    model,\n",
        "    input_shape,\n",
        "    device='cuda',\n",
        "    runs=100,\n",
        "    warmup=20\n",
        "):\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            x = torch.randn(input_shape, device=device)\n",
        "            _ = model(x)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        for _ in range(runs):\n",
        "            x = torch.randn(input_shape, device=device)\n",
        "            elapsed = time_forward(model, x, device)\n",
        "            times.append(elapsed)\n",
        "\n",
        "    times = np.array(times)\n",
        "\n",
        "    return {\n",
        "        \"mean_ms\": times.mean() * 1000,\n",
        "        \"std_ms\": times.std() * 1000,\n",
        "        \"median_ms\": np.median(times) * 1000,\n",
        "        \"min_ms\": times.min() * 1000,\n",
        "        \"max_ms\": times.max() * 1000,\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Batch-N Throughput Benchmark\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark_throughput(\n",
        "    model,\n",
        "    input_shape,\n",
        "    batch_size,\n",
        "    device='cuda',\n",
        "    runs=100,\n",
        "    warmup=20\n",
        "):\n",
        "    model = model.to(device).eval()\n",
        "    shape = (batch_size,) + input_shape[1:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            x = torch.randn(shape, device=device)\n",
        "            _ = model(x)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        for _ in range(runs):\n",
        "            x = torch.randn(shape, device=device)\n",
        "            elapsed = time_forward(model, x, device)\n",
        "            times.append(elapsed)\n",
        "\n",
        "    times = np.array(times)\n",
        "\n",
        "    return {\n",
        "        \"batch_time_ms\": times.mean() * 1000,\n",
        "        \"per_sample_ms\": (times.mean() / batch_size) * 1000,\n",
        "        \"samples_per_sec\": batch_size / times.mean()\n",
        "    }\n",
        "\n",
        "def run_inference_benchmark(\n",
        "    model,\n",
        "    input_shape=(1, 5000, 12),\n",
        "    batch_size=16,\n",
        "    device='cuda'\n",
        "):\n",
        "    setup_benchmark_env()\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"INFERENCE BENCHMARK\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Input shape: {input_shape}\")\n",
        "    print(f\"Throughput batch size: {batch_size}\")\n",
        "\n",
        "    latency = benchmark_latency(\n",
        "        model=model,\n",
        "        input_shape=input_shape,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    throughput = benchmark_throughput(\n",
        "        model=model,\n",
        "        input_shape=input_shape,\n",
        "        batch_size=batch_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"\\nBatch-1 Latency (ms):\")\n",
        "    for k, v in latency.items():\n",
        "        print(f\"  {k:>10}: {v:.3f}\")\n",
        "\n",
        "    print(\"\\nBatch-N Throughput:\")\n",
        "    for k, v in throughput.items():\n",
        "        if \"ms\" in k:\n",
        "            print(f\"  {k:>16}: {v:.3f}\")\n",
        "        else:\n",
        "            print(f\"  {k:>16}: {v:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"latency\": latency,\n",
        "        \"throughput\": throughput\n",
        "    }\n",
        "\n",
        "results = run_inference_benchmark(\n",
        "    model,\n",
        "    input_shape=(1, 5000, 12),\n",
        "    batch_size=16,\n",
        "    device='cpu'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
